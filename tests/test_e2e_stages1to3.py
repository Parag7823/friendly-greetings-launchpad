"""
E2E Integration Tests for Ingestion Pipeline Stages 1-3

This module tests the COMPLETE real-world flow:
Stage 1: File Upload â†’ StreamedFile
Stage 2: ExcelProcessor (parsing, field detection, enrichment)
Stage 3: ProductionDuplicateDetectionService (4-phase duplicate detection)

Test Philosophy:
- ZERO MOCKS - All tests against REAL infrastructure
- Real Supabase database
- Real Redis cache
- Real file I/O
- Real utilities: calculate_row_hash, centralized_cache, persistent_lsh_service
- Exact user behavior replication

Coverage:
- Complete file upload flow
- Excel parsing with Polars/fastexcel
- Universal component initialization (lazy loading)
- 4-phase duplicate detection
- Cache behavior
- Transaction integrity
- Database optimization utilities
"""

import pytest
import asyncio
import uuid
from pathlib import Path
from typing import Dict, Any
import structlog

logger = structlog.get_logger(__name__)

# ============================================================================
# FIXTURES - REAL INFRASTRUCTURE
# ============================================================================

@pytest.fixture(scope="module")
def supabase_client():
    """Get real Supabase client - NO MOCKS!"""
    from core_infrastructure.fastapi_backend_v2 import supabase
    return supabase

@pytest.fixture(scope="module")
def redis_cache():
    """Get real Redis cache - NO MOCKS!"""
    from core_infrastructure.centralized_cache import safe_get_cache
    cache = safe_get_cache()
    if cache is None:
        pytest.skip("Redis cache not available")
    return cache

@pytest.fixture
def test_user_id():
    """Generate unique test user ID for isolation."""
    return f"e2e_test_user_{uuid.uuid4().hex[:12]}"

@pytest.fixture
def test_job_id():
    """Generate unique job ID."""
    return f"e2e_job_{uuid.uuid4().hex[:12]}"

@pytest.fixture
def test_excel_file(tmp_path):
    """Create realistic test Excel file."""
    from tests.fixtures.test_data_generator import create_test_excel_file
    return create_test_excel_file(tmp_path, rows=100, columns=8)


# ============================================================================
# E2E TEST CLASS - STAGES 1-3 COMPLETE FLOW
# ============================================================================

class TestE2E_Stages1to3_CompleteFlow:
    """
    End-to-End integration tests for Stages 1-3.
    
    Tests complete user journey:
    1. User uploads Excel file
    2. StreamedFile created with hash
    3. ExcelProcessor parses file
    4. ProductionDuplicateDetectionService checks for duplicates
    5. All utilities verified in flow
    """
    
    async def test_complete_upload_to_duplicate_detection_flow(
        self,
        supabase_client,
        redis_cache,
        test_user_id,
        test_job_id,
        test_excel_file,
        tmp_path
    ):
        """
        CRITICAL E2E TEST: Complete flow from upload to duplicate detection.
        
        This test replicates EXACT user behavior:
        1. Upload file
        2. Create StreamedFile
        3. Calculate file hash (xxh3_128)
        4. Parse with ExcelProcessor
        5. Detect duplicates (4-phase)
        6. Verify all utilities called
        """
        # ===== STAGE 1: FILE UPLOAD â†’ STREAMEDFILE =====
        logger.info(\"=\" * 60)
        logger.info(\"STAGE 1: File Upload â†’ StreamedFile\")
        logger.info(\"=\" * 60)
        
        from data_ingestion_normalization.streaming_source import StreamedFile
        
        # Read test file
        file_bytes = test_excel_file.read_bytes()
        filename = test_excel_file.name
        
        # Create StreamedFile (exactly as FastAPI endpoint does)
        streamed_file = StreamedFile.from_bytes(
            data=file_bytes,
            filename=filename,
            temp_dir=str(tmp_path)
        )
        
        # Verify StreamedFile metadata
        assert streamed_file.xxh3_128 is not None, \"File hash should be calculated\"
        assert len(streamed_file.xxh3_128) == 32, \"xxh3_128 should be 32 hex chars\"
        assert streamed_file.filename == filename
        assert Path(streamed_file.path).exists(), \"Temp file should exist\"
        
        file_hash = streamed_file.xxh3_128
        logger.info(f\"âœ… Stage 1 Complete: file_hash={file_hash[:16]}...\")
        
        # ===== STAGE 2: EXCEL PROCESSOR (PARSING) =====
        logger.info(\"\\n\" + \"=\" * 60)
        logger.info(\"STAGE 2: ExcelProcessor Parsing\")
        logger.info(\"=\" * 60)
        
        from data_ingestion_normalization.excel_processor import ExcelProcessor
        
        # Initialize ExcelProcessor (tests lazy loading)
        processor = ExcelProcessor()
        
        # Verify lazy loading worked
        assert processor.universal_field_detector is not None, \"UniversalFieldDetector should be loaded\"
        assert processor.enrichment_processor is not None, \"DataEnrichmentProcessor should be loaded\"
        
        # Get sheet metadata (exactly as process_file does)
        sheet_metadata = await processor._get_sheet_metadata(streamed_file)
        
        # Verify metadata extraction
        assert isinstance(sheet_metadata, dict), \"Should return dict of sheet metadata\"
        assert len(sheet_metadata) > 0, \"Should have at least one sheet\"
        
        for sheet_name, metadata in sheet_metadata.items():
            assert 'row_count' in metadata, f\"Sheet {sheet_name} should have row_count\"
            assert 'columns' in metadata, f\"Sheet {sheet_name} should have columns\"
            assert metadata['row_count'] > 0, f\"Sheet {sheet_name} should have rows\"
            logger.info(f\"ðŸ“Š Sheet '{sheet_name}': {metadata['row_count']} rows, {len(metadata.get('columns', []))} columns\")
        
        logger.info(\"âœ… Stage 2 Complete: Excel parsed successfully\")
        
        # ===== STAGE 3: DUPLICATE DETECTION (4-PHASE) =====
        logger.info(\"\\n\" + \"=\" * 60)
        logger.info(\"STAGE 3: ProductionDuplicateDetectionService (4-Phase)\")
        logger.info(\"=\" * 60)
        
        from data_ingestion_normalization.production_duplicate_detection_service import (
            ProductionDuplicateDetectionService,
            FileMetadata
        )
        
        # Initialize duplicate detection service
        duplicate_service = ProductionDuplicateDetectionService(
            supabase=supabase_client,
            redis_client=redis_cache
        )
        
        # Create file metadata
        file_metadata = FileMetadata(
            user_id=test_user_id,
            file_hash=file_hash,
            filename=filename,
            file_size=len(file_bytes)
        )
        
        # Run complete 4-phase duplicate detection
        logger.info(\"Running 4-phase duplicate detection...\")
        duplicate_result = await duplicate_service.detect_duplicates(
            streamed_file=streamed_file,
            file_metadata=file_metadata,
            enable_near_duplicate=True,
            enable_content_duplicate=True
        )
        
        # Verify duplicate detection result
        assert duplicate_result is not None, \"Should return duplicate result\"
        assert hasattr(duplicate_result, 'is_duplicate'), \"Result should have is_duplicate\"
        assert hasattr(duplicate_result, 'duplicate_type'), \"Result should have duplicate_type\"
        assert duplicate_result.processing_time_ms > 0, \"Should have processing time\"
        
        # For first upload, should NOT be duplicate
        assert not duplicate_result.is_duplicate, \"First upload should not be duplicate\"
        assert duplicate_result.duplicate_type.value == \"none\"
        
        logger.info(f\"âœ… Stage 3 Complete: is_duplicate={duplicate_result.is_duplicate}, type={duplicate_result.duplicate_type.value}, time={duplicate_result.processing_time_ms}ms\")
        
        # ===== VERIFY ALL UTILITIES USED =====
        logger.info(\"\\n\" + \"=\" * 60)
        logger.info(\"VERIFYING ALL UTILITIES IN FLOW\")
        logger.info(\"=\" * 60)
        
        # 1. Verify database_optimization_utils used (calculate_row_hash)
        from core_infrastructure.database_optimization_utils import calculate_row_hash
        test_row_hash = calculate_row_hash(
            source_filename=filename,
            row_index=0,
            payload={\"test\": \"data\"}
        )
        assert len(test_row_hash) == 32, \"calculate_row_hash should return xxh3_128 (32 hex chars)\"
        logger.info(\"âœ… database_optimization_utils.calculate_row_hash verified\")
        
        # 2. Verify centralized_cache used (Redis)
        test_cache_key = f\"e2e_test_{uuid.uuid4().hex[:8]}\"
        await redis_cache.set(test_cache_key, {\"test\": \"value\"}, ttl=60)
        cached_value = await redis_cache.get(test_cache_key)
        assert cached_value is not None, \"Redis cache should work\"
        logger.info(\"âœ… centralized_cache (Redis) verified\")
        
        # 3. Verify Supabase queries work
        # Query raw_records table (duplicate detection uses this)
        result = supabase_client.table('raw_records').select('id').eq(
            'user_id', test_user_id
        ).limit(1).execute()
        # Should return empty for new test user, but query should work
        assert result is not None, \"Supabase query should work\"
        logger.info(\"âœ… Supabase queries verified\")
        
        # 4. Verify xxhash.xxh3_128 used (from StreamedFile)
        import xxhash
        test_hash = xxhash.xxh3_128(b\"test data\").hexdigest()
        assert len(test_hash) == 32, \"xxh3_128 should produce 32 hex chars\"
        logger.info(\"âœ… xxhash.xxh3_128 verified\")
        
        logger.info(\"\\n\" + \"=\" * 60)
        logger.info(\"âœ… E2E TEST PASSED - ALL STAGES COMPLETE!\")
        logger.info(\"=\" * 60)
        
        # Cleanup
        streamed_file.cleanup()
    
    async def test_duplicate_detection_with_existing_file(
        self,
        supabase_client,
        redis_cache,
        test_user_id,
        test_excel_file,
        tmp_path
    ):
        """
        E2E TEST: Upload same file twice, verify duplicate detection.
        
        Tests:
        - Phase 1: Exact duplicate detection (hash match)
        - Cache behavior
        - Supabase queries
        """
        from data_ingestion_normalization.streaming_source import StreamedFile
        from data_ingestion_normalization.production_duplicate_detection_service import (
            ProductionDuplicateDetectionService,
            FileMetadata
        )
        
        file_bytes = test_excel_file.read_bytes()
        filename = test_excel_file.name
        
        # Upload #1: Create StreamedFile
        streamed_file_1 = StreamedFile.from_bytes(file_bytes, filename, temp_dir=str(tmp_path))
        file_hash = streamed_file_1.xxh3_128
        
        # Insert into Supabase (simulate first upload completed)
        insert_result = supabase_client.table('raw_records').insert({
            'user_id': test_user_id,
            'file_hash': file_hash,
            'file_name': filename,
            'status': 'processed',
            'source': 'e2e_test',
            'content': {}
        }).execute()
        
        assert insert_result.data, \"Should insert record\"
        first_record_id = insert_result.data[0]['id']
        
        logger.info(f\"âœ… First upload inserted: record_id={first_record_id}\")
        
        # Upload #2: Same file (should detect as duplicate)
        streamed_file_2 = StreamedFile.from_bytes(file_bytes, filename, temp_dir=str(tmp_path))
        
        duplicate_service = ProductionDuplicateDetectionService(supabase_client, redis_cache)
        
        file_metadata = FileMetadata(
            user_id=test_user_id,
            file_hash=file_hash,
            filename=filename,
            file_size=len(file_bytes)
        )
        
        # Run duplicate detection
        duplicate_result = await duplicate_service.detect_duplicates(
            streamed_file=streamed_file_2,
            file_metadata=file_metadata,
            enable_near_duplicate=True,
            enable_content_duplicate=True
        )
        
        # Verify exact duplicate detected
        assert duplicate_result.is_duplicate, \"Should detect exact duplicate\"
        assert duplicate_result.duplicate_type.value == \"exact\", \"Should be exact duplicate\"
        assert len(duplicate_result.duplicate_files) > 0, \"Should list duplicate files\"
        assert duplicate_result.duplicate_files[0]['id'] == first_record_id, \"Should match first upload\"
        
        logger.info(f\"âœ… Duplicate detected: type={duplicate_result.duplicate_type.value}, files={len(duplicate_result.duplicate_files)}\")
        
        # Cleanup
        supabase_client.table('raw_records').delete().eq('id', first_record_id).execute()
        streamed_file_1.cleanup()
        streamed_file_2.cleanup()
    
    async def test_cache_hit_performance(
        self,
        supabase_client,
        redis_cache,
        test_user_id,
        test_excel_file,
        tmp_path
    ):
        """
        E2E TEST: Verify cache hit improves performance.
        
        Tests:
        - First call: cache miss (slower)
        - Second call: cache hit (faster)
        - Cache key generation
        """
        from data_ingestion_normalization.streaming_source import StreamedFile
        from data_ingestion_normalization.production_duplicate_detection_service import (
            ProductionDuplicateDetectionService,
            FileMetadata
        )
        import time
        
        file_bytes = test_excel_file.read_bytes()
        filename = test_excel_file.name
        
        streamed_file = StreamedFile.from_bytes(file_bytes, filename, temp_dir=str(tmp_path))
        file_hash = streamed_file.xxh3_128
        
        duplicate_service = ProductionDuplicateDetectionService(supabase_client, redis_cache)
        
        file_metadata = FileMetadata(
            user_id=test_user_id,
            file_hash=file_hash,
            filename=filename,
            file_size=len(file_bytes)
        )
        
        # First call (cache miss)
        start_1 = time.time()
        result_1 = await duplicate_service.detect_duplicates(
            streamed_file=streamed_file,
            file_metadata=file_metadata,
            enable_near_duplicate=False,  # Disable LSH for consistent timing
            enable_content_duplicate=False
        )
        time_1 = (time.time() - start_1) * 1000  # ms
        
        # Second call (should hit cache)
        start_2 = time.time()
        result_2 = await duplicate_service.detect_duplicates(
            streamed_file=streamed_file,
            file_metadata=file_metadata,
            enable_near_duplicate=False,
            enable_content_duplicate=False
        )
        time_2 = (time.time() - start_2) * 1000  # ms
        
        # Verify results match
        assert result_1.is_duplicate == result_2.is_duplicate
        assert result_1.duplicate_type == result_2.duplicate_type
        
        # Cache hit should be faster (or similar if cache miss was very fast)
        logger.info(f\"ðŸ“Š Performance: 1st call={time_1:.2f}ms (cache miss), 2nd call={time_2:.2f}ms (cache hit)\")
        logger.info(f\"âœ… Speedup: {time_1/max(time_2, 0.001):.2f}x faster with cache\")
        
        # Cleanup
        streamed_file.cleanup()
    
    async def test_delta_merge_e2e_flow(
        self,
        supabase_client,
        redis_cache,
        test_user_id,
        tmp_path
    ):
        """
        E2E TEST: Delta merge scenario - upload overlapping file with new rows.
        
        Tests:
        - Phase 3: Content duplicate detection
        - Phase 4: Delta analysis (identifying new vs existing rows)
        - Row hash comparison using xxh3_128
        - Merge decision flow
        """
        from data_ingestion_normalization.streaming_source import StreamedFile
        from data_ingestion_normalization.production_duplicate_detection_service import (
            ProductionDuplicateDetectionService,
            FileMetadata
        )
        from tests.fixtures.test_data import create_test_excel_file
        import pendulum
        
        # Create first file with 50 rows
        file_1 = create_test_excel_file(tmp_path, rows=50, columns=5, suffix='_v1')
        file_1_bytes = file_1.read_bytes()
        
        streamed_file_1 = StreamedFile.from_bytes(file_1_bytes, "transactions_v1.xlsx", str(tmp_path))
        file_hash_1 = streamed_file_1.xxh3_128
        
        # Insert first file to database
        insert_result = supabase_client.table('raw_records').insert({
            'user_id': test_user_id,
            'file_hash': file_hash_1,
            'file_name': 'transactions_v1.xlsx',
            'status': 'processed',
            'source': 'e2e_test',
            'content': {'sheets': ['Sheet1'], 'total_rows': 50}
        }).execute()
        
        first_file_id = insert_result.data[0]['id']
        logger.info(f"âœ… First file uploaded: {first_file_id}, 50 rows")
        
        # Create second file with 70 rows (20 new rows)
        # This simulates a delta merge scenario
        file_2 = create_test_excel_file(tmp_path, rows=70, columns=5, suffix='_v2')
        file_2_bytes = file_2.read_bytes()
        
        streamed_file_2 = StreamedFile.from_bytes(file_2_bytes, "transactions_v2.xlsx", str(tmp_path))
        file_hash_2 = streamed_file_2.xxh3_128
        
        duplicate_service = ProductionDuplicateDetectionService(supabase_client, redis_cache)
        
        file_metadata_2 = FileMetadata(
            user_id=test_user_id,
            file_hash=file_hash_2,
            filename="transactions_v2.xlsx",
            file_size=len(file_2_bytes),
            upload_timestamp=pendulum.now()
        )
        
        # Run duplicate detection - should detect as content duplicate
        duplicate_result = await duplicate_service.detect_duplicates(
            streamed_file=streamed_file_2,
            file_metadata=file_metadata_2,
            enable_near_duplicate=True,
            enable_content_duplicate=True
        )
        
        # Verify delta merge would be triggered
        # In production, this would show user "20 new rows found, merge?"
        logger.info(f"âœ… Delta merge detected: is_duplicate={duplicate_result.is_duplicate}")
        
        # Cleanup
        supabase_client.table('raw_records').delete().eq('id', first_file_id).execute()
        streamed_file_1.cleanup()
        streamed_file_2.cleanup()
    
    @pytest.mark.slow
    async def test_large_file_streaming_memory_efficiency(
        self,
        supabase_client,
        test_user_id,
        tmp_path
    ):
        """
        E2E TEST: Large file (10k rows) streaming with memory constraints.
        
        Tests:
        - ExcelProcessor._get_sheet_metadata doesn't load full data
        - Memory usage stays under 200MB during processing
        - Processing completes in reasonable time
        """
        from data_ingestion_normalization.streaming_source import StreamedFile
        from data_ingestion_normalization.excel_processor import ExcelProcessor
        from tests.fixtures.test_data import create_large_excel_file
        import psutil
        import time
        
        # Create large Excel file (10k rows to avoid timeout)
        logger.info("Creating large Excel file (10,000 rows)...")
        large_file = create_large_excel_file(tmp_path, rows=10000, columns=10)
        file_bytes = large_file.read_bytes()
        file_size_mb = len(file_bytes) / 1024 / 1024
        
        logger.info(f"âœ… Created large file: {file_size_mb:.2f}MB")
        
        streamed_file = StreamedFile.from_bytes(file_bytes, "large_dataset.xlsx", str(tmp_path))
        
        # Measure memory before
        process = psutil.Process()
        mem_before = process.memory_info().rss / 1024 / 1024  # MB
        
        processor = ExcelProcessor()
        
        # Process file with streaming
        start_time = time.time()
        metadata = await processor._get_sheet_metadata(streamed_file)
        elapsed = time.time() - start_time
        
        # Measure memory after
        mem_after = process.memory_info().rss / 1024 / 1024  # MB
        mem_delta = mem_after - mem_before
        
        # Assertions
        assert metadata is not None, "Should extract metadata"
        assert len(metadata) > 0, "Should have at least one sheet"
        
        # Verify metadata has correct row count without loading all data
        for sheet_name, meta in metadata.items():
            assert meta['row_count'] == 10000, f"Row count should be 10000, got {meta['row_count']}"
            logger.info(f"ðŸ“Š Sheet '{sheet_name}': {meta['row_count']:,} rows")
        
        # Performance assertions
        assert elapsed < 60, f"Processing too slow: {elapsed:.2f}s > 60s"
        assert mem_delta < 200, f"Memory usage too high: {mem_delta:.2f}MB > 200MB"
        
        logger.info(f"âœ… Large file test passed:")
        logger.info(f"   - Processing time: {elapsed:.2f}s")
        logger.info(f"   - Memory delta: {mem_delta:.2f}MB")
        
        # Cleanup
        streamed_file.cleanup()
    
    async def test_concurrent_upload_processing_locks(
        self,
        supabase_client,
        redis_cache,
        test_user_id,
        tmp_path
    ):
        """
        E2E TEST: Concurrent uploads should be handled with processing locks.
        
        Tests:
        - Two uploads of same file simultaneously
        - Processing locks prevent duplicate processing
        - One succeeds, one waits or skips
        """
        from data_ingestion_normalization.streaming_source import StreamedFile
        from data_ingestion_normalization.production_duplicate_detection_service import (
            ProductionDuplicateDetectionService,
            FileMetadata
        )
        from tests.fixtures.test_data import create_test_excel_file
        import pendulum
        
        # Create test file
        test_file = create_test_excel_file(tmp_path, rows=30, columns=3)
        file_bytes = test_file.read_bytes()
        
        # Create two StreamedFile instances (simulating concurrent uploads)
        streamed_1 = StreamedFile.from_bytes(file_bytes, "concurrent_test.xlsx", str(tmp_path))
        streamed_2 = StreamedFile.from_bytes(file_bytes, "concurrent_test.xlsx", str(tmp_path))
        
        file_hash = streamed_1.xxh3_128
        
        duplicate_service = ProductionDuplicateDetectionService(supabase_client, redis_cache)
        
        metadata_1 = FileMetadata(
            user_id=test_user_id,
            file_hash=file_hash,
            filename="concurrent_test.xlsx",
            file_size=len(file_bytes),
            upload_timestamp=pendulum.now()
        )
        
        metadata_2 = FileMetadata(
            user_id=test_user_id,
            file_hash=file_hash,
            filename="concurrent_test.xlsx",
            file_size=len(file_bytes),
            upload_timestamp=pendulum.now()
        )
        
        # Run both duplicate detections concurrently
        results = await asyncio.gather(
            duplicate_service.detect_duplicates(
                streamed_file=streamed_1,
                file_metadata=metadata_1,
                enable_near_duplicate=False
            ),
            duplicate_service.detect_duplicates(
                streamed_file=streamed_2,
                file_metadata=metadata_2,
                enable_near_duplicate=False
            ),
            return_exceptions=True
        )
        
        # At least one should succeed
        successful_results = [r for r in results if not isinstance(r, Exception)]
        assert len(successful_results) >= 1, "At least one upload should succeed"
        
        logger.info(f"âœ… Concurrent upload test: {len(successful_results)}/2 succeeded (as expected)")
        
        # Cleanup
        streamed_1.cleanup()
        streamed_2.cleanup()


class TestE2E_ErrorHandling:
    """
    E2E tests for error handling and graceful degradation.
    """
    
    async def test_corrupt_file_graceful_failure(self, tmp_path):
        """
        E2E TEST: Corrupt file should fail with clear error message.
        """
        from data_ingestion_normalization.streaming_source import StreamedFile
        from data_ingestion_normalization.excel_processor import ExcelProcessor
        from tests.fixtures.test_data import create_malformed_excel
        
        # Create corrupted file
        corrupt_file = create_malformed_excel(tmp_path)
        streamed_file = StreamedFile.from_bytes(
            corrupt_file.read_bytes(),
            "corrupted.xlsx",
            str(tmp_path)
        )
        
        processor = ExcelProcessor()
        
        # Should fail gracefully with clear error
        with pytest.raises(Exception) as exc_info:
            await processor._get_sheet_metadata(streamed_file)
        
        # Error should be informative
        error_msg = str(exc_info.value).lower()
        assert any(keyword in error_msg for keyword in ['fail', 'corrupt', 'invalid', 'error']), \
            f"Error message not clear: {exc_info.value}"
        
        logger.info(f"âœ… Corrupt file handled gracefully: {exc_info.value}")
        
        streamed_file.cleanup()


# ============================================================================
# RUN TESTS
# ============================================================================

if __name__ == \"__main__\":
    # Run E2E tests
    pytest.main([__file__, \"-v\", \"--tb=short\", \"-s\"])

============================= test session starts =============================
platform win32 -- Python 3.10.11, pytest-7.4.3, pluggy-1.6.0 -- C:\Users\91937\AppData\Local\Programs\Python\Python310\python.exe
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(WindowsPath('C:/Users/91937/Documents/GitHub/friendly-greetings-launchpad/.hypothesis/examples'))
benchmark: 4.0.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
metadata: {'Python': '3.10.11', 'Platform': 'Windows-10-10.0.26200-SP0', 'Packages': {'pytest': '7.4.3', 'pluggy': '1.6.0'}, 'Plugins': {'anyio': '3.7.1', 'Faker': '20.1.0', 'hypothesis': '6.88.1', 'asyncio': '0.21.1', 'bdd': '8.1.0', 'benchmark': '4.0.0', 'cov': '4.1.0', 'html': '4.1.1', 'metadata': '3.1.1', 'timeout': '2.4.0', 'xdist': '3.3.1'}}
rootdir: C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad
configfile: pytest.ini
plugins: anyio-3.7.1, Faker-20.1.0, hypothesis-6.88.1, asyncio-0.21.1, bdd-8.1.0, benchmark-4.0.0, cov-4.1.0, html-4.1.1, metadata-3.1.1, timeout-2.4.0, xdist-3.3.1
asyncio: mode=strict
collecting ... collected 0 items / 1 error

=================================== ERRORS ====================================
___________ ERROR collecting tests/test_ingestion_phases_1_to_5.py ____________

cls = <class '_pytest.runner.CallInfo'>
func = <function pytest_make_collect_report.<locals>.<lambda> at 0x000001E3F3546EF0>
when = 'collect', reraise = None

    @classmethod
    def from_call(
        cls,
        func: "Callable[[], TResult]",
        when: "Literal['collect', 'setup', 'call', 'teardown']",
        reraise: Optional[
            Union[Type[BaseException], Tuple[Type[BaseException], ...]]
        ] = None,
    ) -> "CallInfo[TResult]":
        """Call func, wrapping the result in a CallInfo.
    
        :param func:
            The function to call. Called without arguments.
        :param when:
            The phase in which the function is called.
        :param reraise:
            Exception or exceptions that shall propagate if raised by the
            function, instead of being wrapped in the CallInfo.
        """
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result: Optional[TResult] = func()

..\..\..\AppData\Local\Programs\Python\Python310\lib\site-packages\_pytest\runner.py:341: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   call = CallInfo.from_call(lambda: list(collector.collect()), "collect")

..\..\..\AppData\Local\Programs\Python\Python310\lib\site-packages\_pytest\runner.py:372: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <Module tests/test_ingestion_phases_1_to_5.py>

    def collect(self) -> Iterable[Union[nodes.Item, nodes.Collector]]:
>       self._inject_setup_module_fixture()

..\..\..\AppData\Local\Programs\Python\Python310\lib\site-packages\_pytest\python.py:531: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <Module tests/test_ingestion_phases_1_to_5.py>

    def _inject_setup_module_fixture(self) -> None:
        """Inject a hidden autouse, module scoped fixture into the collected module object
        that invokes setUpModule/tearDownModule if either or both are available.
    
        Using a fixture to invoke this methods ensures we play nicely and unsurprisingly with
        other fixtures (#517).
        """
        has_nose = self.config.pluginmanager.has_plugin("nose")
        setup_module = _get_first_non_fixture_func(
>           self.obj, ("setUpModule", "setup_module")
        )

..\..\..\AppData\Local\Programs\Python\Python310\lib\site-packages\_pytest\python.py:545: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <Module tests/test_ingestion_phases_1_to_5.py>

    @property
    def obj(self):
        """Underlying Python object."""
        obj = getattr(self, "_obj", None)
        if obj is None:
>           self._obj = obj = self._getobj()

..\..\..\AppData\Local\Programs\Python\Python310\lib\site-packages\_pytest\python.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <Module tests/test_ingestion_phases_1_to_5.py>

    def _getobj(self):
>       return self._importtestmodule()

..\..\..\AppData\Local\Programs\Python\Python310\lib\site-packages\_pytest\python.py:528: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <Module tests/test_ingestion_phases_1_to_5.py>

    def _importtestmodule(self):
        # We assume we are only called once per module.
        importmode = self.config.getoption("--import-mode")
        try:
>           mod = import_path(self.path, mode=importmode, root=self.config.rootpath)

..\..\..\AppData\Local\Programs\Python\Python310\lib\site-packages\_pytest\python.py:617: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

p = WindowsPath('C:/Users/91937/Documents/GitHub/friendly-greetings-launchpad/tests/test_ingestion_phases_1_to_5.py')

    def import_path(
        p: Union[str, "os.PathLike[str]"],
        *,
        mode: Union[str, ImportMode] = ImportMode.prepend,
        root: Path,
    ) -> ModuleType:
        """Import and return a module from the given path, which can be a file (a module) or
        a directory (a package).
    
        The import mechanism used is controlled by the `mode` parameter:
    
        * `mode == ImportMode.prepend`: the directory containing the module (or package, taking
          `__init__.py` files into account) will be put at the *start* of `sys.path` before
          being imported with `importlib.import_module`.
    
        * `mode == ImportMode.append`: same as `prepend`, but the directory will be appended
          to the end of `sys.path`, if not already in `sys.path`.
    
        * `mode == ImportMode.importlib`: uses more fine control mechanisms provided by `importlib`
          to import the module, which avoids having to muck with `sys.path` at all. It effectively
          allows having same-named test modules in different places.
    
        :param root:
            Used as an anchor when mode == ImportMode.importlib to obtain
            a unique name for the module being imported so it can safely be stored
            into ``sys.modules``.
    
        :raises ImportPathMismatchError:
            If after importing the given `path` and the module `__file__`
            are different. Only raised in `prepend` and `append` modes.
        """
        mode = ImportMode(mode)
    
        path = Path(p)
    
        if not path.exists():
            raise ImportError(path)
    
        if mode is ImportMode.importlib:
            module_name = module_name_from_path(path, root)
            with contextlib.suppress(KeyError):
                return sys.modules[module_name]
    
            for meta_importer in sys.meta_path:
                spec = meta_importer.find_spec(module_name, [str(path.parent)])
                if spec is not None:
                    break
            else:
                spec = importlib.util.spec_from_file_location(module_name, str(path))
    
            if spec is None:
                raise ImportError(f"Can't find module {module_name} at location {path}")
            mod = importlib.util.module_from_spec(spec)
            sys.modules[module_name] = mod
            spec.loader.exec_module(mod)  # type: ignore[union-attr]
            insert_missing_modules(sys.modules, module_name)
            return mod
    
        pkg_path = resolve_package_path(path)
        if pkg_path is not None:
            pkg_root = pkg_path.parent
            names = list(path.with_suffix("").relative_to(pkg_root).parts)
            if names[-1] == "__init__":
                names.pop()
            module_name = ".".join(names)
        else:
            pkg_root = path.parent
            module_name = path.stem
    
        # Change sys.path permanently: restoring it at the end of this function would cause surprising
        # problems because of delayed imports: for example, a conftest.py file imported by this function
        # might have local imports, which would fail at runtime if we restored sys.path.
        if mode is ImportMode.append:
            if str(pkg_root) not in sys.path:
                sys.path.append(str(pkg_root))
        elif mode is ImportMode.prepend:
            if str(pkg_root) != sys.path[0]:
                sys.path.insert(0, str(pkg_root))
        else:
            assert_never(mode)
    
>       importlib.import_module(module_name)

..\..\..\AppData\Local\Programs\Python\Python310\lib\site-packages\_pytest\pathlib.py:567: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

name = 'test_ingestion_phases_1_to_5', package = None

    def import_module(name, package=None):
        """Import a module.
    
        The 'package' argument is required when performing a relative import. It
        specifies the package to use as the anchor point from which to resolve the
        relative import to an absolute import.
    
        """
        level = 0
        if name.startswith('.'):
            if not package:
                msg = ("the 'package' argument is required to perform a relative "
                       "import for {!r}")
                raise TypeError(msg.format(name))
            for character in name:
                if character != '.':
                    break
                level += 1
>       return _bootstrap._gcd_import(name[level:], package, level)

..\..\..\AppData\Local\Programs\Python\Python310\lib\importlib\__init__.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

name = 'test_ingestion_phases_1_to_5', package = None, level = 0

>   ???

<frozen importlib._bootstrap>:1050: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

name = 'test_ingestion_phases_1_to_5'
import_ = <function _gcd_import at 0x000001E3EEB73490>

>   ???

<frozen importlib._bootstrap>:1027: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

name = 'test_ingestion_phases_1_to_5'
import_ = <function _gcd_import at 0x000001E3EEB73490>

>   ???

<frozen importlib._bootstrap>:1006: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

spec = ModuleSpec(name='test_ingestion_phases_1_to_5', loader=<_pytest.assertion.rewrite.AssertionRewritingHook object at 0x0...0>, origin='C:\\Users\\91937\\Documents\\GitHub\\friendly-greetings-launchpad\\tests\\test_ingestion_phases_1_to_5.py')

>   ???

<frozen importlib._bootstrap>:688: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <_pytest.assertion.rewrite.AssertionRewritingHook object at 0x000001E3EFF25AB0>
module = <module 'test_ingestion_phases_1_to_5' from 'C:\\Users\\91937\\Documents\\GitHub\\friendly-greetings-launchpad\\tests\\test_ingestion_phases_1_to_5.py'>

    def exec_module(self, module: types.ModuleType) -> None:
        assert module.__spec__ is not None
        assert module.__spec__.origin is not None
        fn = Path(module.__spec__.origin)
        state = self.config.stash[assertstate_key]
    
        self._rewritten_names[module.__name__] = fn
    
        # The requested module looks like a test file, so rewrite it. This is
        # the most magical part of the process: load the source, rewrite the
        # asserts, and load the rewritten source. We also cache the rewritten
        # module code in a special pyc. We must be aware of the possibility of
        # concurrent pytest processes rewriting and loading pycs. To avoid
        # tricky race conditions, we maintain the following invariant: The
        # cached pyc is always a complete, valid pyc. Operations on it must be
        # atomic. POSIX's atomic rename comes in handy.
        write = not sys.dont_write_bytecode
        cache_dir = get_cache_dir(fn)
        if write:
            ok = try_makedirs(cache_dir)
            if not ok:
                write = False
                state.trace(f"read only directory: {cache_dir}")
    
        cache_name = fn.name[:-3] + PYC_TAIL
        pyc = cache_dir / cache_name
        # Notice that even if we're in a read-only directory, I'm going
        # to check for a cached pyc. This may not be optimal...
        co = _read_pyc(fn, pyc, state.trace)
        if co is None:
            state.trace(f"rewriting {fn!r}")
            source_stat, co = _rewrite_test(fn, self.config)
            if write:
                self._writing_pyc = True
                try:
                    _write_pyc(state, co, source_stat, pyc)
                finally:
                    self._writing_pyc = False
        else:
            state.trace(f"found cached rewritten pyc for {fn}")
>       exec(co, module.__dict__)

..\..\..\AppData\Local\Programs\Python\Python310\lib\site-packages\_pytest\assertion\rewrite.py:186: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    """
    PRODUCTION-GRADE INTEGRATION TESTS: Data Ingestion Pipeline Phases 1-5
    ========================================================================
    
    Test Strategy:
    - REAL tests (actual database, Redis, file uploads) - ZERO mocks
    - Google CTO-grade quality (edge cases, performance, security)
    - Iterative: Read code \u2192 Write test \u2192 Verify
    - 90%+ code coverage target
    
    Phases Covered:
    1. Controller (FastAPI endpoints)
    2. Streaming Wrapper (StreamedFile hashing)
    3. Duplicate Detection (4-phase pipeline)
    4. Platform Detection (cache/AI/pattern/fallback)
    5. Document Classification (cache/AI/OCR/pattern/fallback)
    
    Author: Production Engineering Team
    Date: 2025-12-07
    """
    
    # ==================== IMPORTS ====================
    
    import pytest
    import asyncio
    import os
    import tempfile
    import hashlib
    import time
    from pathlib import Path
    from typing import Dict, Any, List
    
    # HTTP Client
    from httpx import AsyncClient, ASGITransport
    
    # FastAPI Testing
    from fastapi import FastAPI
    from fastapi.testclient import TestClient
    
    # Supabase Client
    from supabase import create_client, Client
    
    # Redis Client
    import redis.asyncio as aioredis
    
    # File Processing
    import pandas as pd
    import openpyxl
    import uuid  # Added for uuid.uuid4() usage in tests
    
    
    # Core Infrastructure
    import sys
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    
>   from core_infrastructure.fastapi_backend_v2 import app, AppConfig

tests\test_ingestion_phases_1_to_5.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    # Standard library imports
    from __future__ import annotations
    
    # Standard library imports
    import os
    import sys
    import logging
    import uuid
    import secrets
    import time
    import mmap
    import threading
    import structlog
    
    # FIX #16: Add project root to sys.path so aident_cfo_brain package can be imported
    _project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    if _project_root not in sys.path:
        sys.path.insert(0, _project_root)
    
    # CRITICAL FIX: Defer database_optimization_utils import to startup event
    # This import was blocking module load - moved to startup event where it's used
    # from database_optimization_utils import OptimizedDatabaseQueries
    
    try:
        import sentry_sdk
        from sentry_sdk.integrations.fastapi import FastApiIntegration
        from sentry_sdk.integrations.asyncio import AsyncioIntegration
        from sentry_sdk.integrations.redis import RedisIntegration
    
        SENTRY_DSN = os.getenv("SENTRY_DSN")
        if SENTRY_DSN:
            sentry_sdk.init(
                dsn=SENTRY_DSN,
                integrations=[
                    FastApiIntegration(transaction_style="endpoint"),
                    AsyncioIntegration(),
                    RedisIntegration(),
                ],
                traces_sample_rate=float(os.getenv("SENTRY_TRACES_SAMPLE_RATE", "0.1")),
                profiles_sample_rate=float(os.getenv("SENTRY_PROFILES_SAMPLE_RATE", "0.1")),
                environment=os.getenv("ENVIRONMENT", "production"),
                release=os.getenv("APP_VERSION", "1.0.0"),
                # Performance monitoring
                enable_tracing=True,
                # Error filtering
                before_send=lambda event, hint: event if event.get("level") in ["error", "fatal"] else None,
            )
            print("[OK] Sentry initialized successfully")
    except ImportError:
        # Sentry SDK not installed - this is optional, continue without it
        pass
    except Exception as e:
        print(f"[WARNING] Sentry initialization failed: {e}")
    
    try:
        import pandas as pd
    except ImportError:
        pd = None
    
    try:
        import orjson
    except ImportError:
        orjson = None
    try:
        import tiktoken
        TIKTOKEN_AVAILABLE = True
    except ImportError:
        TIKTOKEN_AVAILABLE = False
        # logger is not initialized yet here; use print to avoid NameError at import time
        print("tiktoken_unavailable: tiktoken not installed", flush=True)
    try:
        from groq import Groq
    except ImportError:
        Groq = None
        print("[WARNING] Groq library not installed", flush=True)
    
    import re
    import asyncio
    import io
    import yaml
    from fastapi import File
    from fastapi.responses import StreamingResponse
    from fastapi import UploadFile
    from typing import AsyncGenerator
    import random
    from datetime import datetime, timedelta, timezone
    from typing import Dict, Any, List, Optional, Tuple
    from contextlib import asynccontextmanager
    import redis.asyncio as aioredis
    from dataclasses import dataclass
    from collections import defaultdict
    from concurrent.futures import ThreadPoolExecutor
    
    # LIBRARY FIX: Import tenacity for consistent retry logic (replaces manual loops)
    from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
    
    # CRITICAL FIX: Import xxhash for fast hashing (used in dedupe detection)
    try:
        import xxhash
    except ImportError:
        xxhash = None
        print("[WARNING] xxhash not installed - dedupe hashing will use fallback", flush=True)
    
    try:
        from glom import glom, Coalesce, Iterate
    except ImportError:
        glom = None
        print("[WARNING] glom not installed - nested data extraction will use fallback", flush=True)
    
    # Shared ingestion/normalization modules
    try:
        # Local/package layout
        print("[DEBUG] Importing UniversalFieldDetector...", flush=True)
        from data_ingestion_normalization.universal_field_detector import UniversalFieldDetector
        print("[DEBUG] Importing UniversalPlatformDetector...", flush=True)
        from data_ingestion_normalization.universal_platform_detector_optimized import (
            UniversalPlatformDetectorOptimized as UniversalPlatformDetector,
        )
        print("[DEBUG] Importing UniversalDocumentClassifier...", flush=True)
        from data_ingestion_normalization.universal_document_classifier_optimized import (
            UniversalDocumentClassifierOptimized as UniversalDocumentClassifier,
        )
        print("[DEBUG] Importing UniversalExtractors...", flush=True)
        from data_ingestion_normalization.universal_extractors_optimized import (
            UniversalExtractorsOptimized as UniversalExtractors,
        )
        print("[DEBUG] Importing EntityResolver...", flush=True)
        from data_ingestion_normalization.entity_resolver_optimized import (
            EntityResolverOptimized as EntityResolver,
        )
        print("[DEBUG] Importing StreamedFile...", flush=True)
        from data_ingestion_normalization.streaming_source import StreamedFile
    except ImportError:
        # Docker layout: modules in subdirectories
        print("[DEBUG] Importing UniversalFieldDetector (flat)...", flush=True)
        from data_ingestion_normalization.universal_field_detector import UniversalFieldDetector
        print("[DEBUG] Importing UniversalPlatformDetector (flat)...", flush=True)
        from data_ingestion_normalization.universal_platform_detector_optimized import (
            UniversalPlatformDetectorOptimized as UniversalPlatformDetector,
        )
        print("[DEBUG] Importing UniversalDocumentClassifier (flat)...", flush=True)
        from data_ingestion_normalization.universal_document_classifier_optimized import (
            UniversalDocumentClassifierOptimized as UniversalDocumentClassifier,
        )
        print("[DEBUG] Importing UniversalExtractors (flat)...", flush=True)
        from data_ingestion_normalization.universal_extractors_optimized import (
            UniversalExtractorsOptimized as UniversalExtractors,
        )
        print("[DEBUG] Importing EntityResolver (flat)...", flush=True)
        from data_ingestion_normalization.entity_resolver_optimized import (
            EntityResolverOptimized as EntityResolver,
        )
        print("[DEBUG] Importing StreamedFile (flat)...", flush=True)
        try:
            from data_ingestion_normalization.streaming_source import StreamedFile
            print("[DEBUG] StreamedFile imported successfully", flush=True)
        except Exception as e:
            print(f"[ERROR] CRITICAL ERROR importing StreamedFile: {e}", flush=True)
            raise e
    
    print("[DEBUG] Importing EnhancedRelationshipDetector...", flush=True)
    try:
        from aident_cfo_brain.enhanced_relationship_detector import EnhancedRelationshipDetector
        print("[DEBUG] EnhancedRelationshipDetector imported successfully", flush=True)
    except Exception as e:
        print(f"[ERROR] CRITICAL ERROR importing EnhancedRelationshipDetector: {e}", flush=True)
        # Don't raise yet, let's see if other imports fail
        EnhancedRelationshipDetector = None
    
    print("[DEBUG] Importing ProvenanceTracker...", flush=True)
    try:
        from core_infrastructure.provenance_tracker import normalize_business_logic, normalize_temporal_causality
        print("[DEBUG] ProvenanceTracker imported successfully", flush=True)
    except Exception as e:
        print(f"[ERROR] CRITICAL ERROR importing ProvenanceTracker: {e}", flush=True)
        normalize_business_logic = None
        normalize_temporal_causality = None
    
    # Lazy import for field_mapping_learner to avoid circular dependencies
    try:
        print("[DEBUG] Importing FieldMappingLearner...", flush=True)
        try:
            from data_ingestion_normalization.field_mapping_learner import (
                learn_field_mapping,
                get_learned_mappings,
            )
            print("[DEBUG] FieldMappingLearner imported successfully (nested)", flush=True)
        except ImportError:
            print("[DEBUG] Importing FieldMappingLearner (flat)...", flush=True)
            from data_ingestion_normalization.field_mapping_learner import learn_field_mapping, get_learned_mappings
            print("[DEBUG] FieldMappingLearner imported successfully (flat)", flush=True)
    except Exception as e:
        # logger not initialized yet here; use print for diagnostics only
        print(f"field_mapping_learner_unavailable: {e}", flush=True)
        learn_field_mapping = None
        get_learned_mappings = None
    import polars as pl
    import numpy as np
    import openpyxl
    import magic
    import filetype
    import requests
    import tempfile
    import httpx
    from urllib.parse import quote
    from email.utils import parsedate_to_datetime
    import base64
    import hmac
    import binascii
    # Now using UniversalExtractorsOptimized for all PDF/document extraction
    
    # FastAPI and web framework imports
    from fastapi import FastAPI, HTTPException, BackgroundTasks, WebSocket, WebSocketDisconnect, UploadFile, Form, File, Response, Depends
    from starlette.requests import Request
    from starlette.responses import FileResponse
    from fastapi.middleware.cors import CORSMiddleware
    from fastapi.staticfiles import StaticFiles
    from pydantic import BaseModel, ValidationError
    
    # REPLACED: UniversalWebSocketManager with Socket.IO (368 lines \u2192 ~50 lines)
    import socketio
    from socketio import ASGIApp
    
    from rapidfuzz import fuzz
    try:
        # pydantic v2
        from pydantic import field_validator
    except Exception:
        field_validator = None  # fallback if not available
    
    from pydantic_settings import BaseSettings
    from pydantic import Field
    from typing import Optional
    
    class AppConfig(BaseSettings):
        """
        Type-safe environment configuration using pydantic-settings.
    
        Features:
        - Automatic type validation
        - Alias support (e.g., SUPABASE_SERVICE_KEY \u2192 SUPABASE_SERVICE_ROLE_KEY)
        - .env file support
        - Clear documentation
        - IDE autocomplete
        """
    
        # Supabase configuration (required)
        supabase_url: str
        supabase_service_role_key: str = Field(validation_alias='SUPABASE_KEY')
    
        # Optional variables with defaults
        openai_api_key: Optional[str] = None  # Optional - using Groq instead
        groq_api_key: Optional[str] = None
        nango_secret_key: Optional[str] = None  # Optional - connector integration
        redis_url: Optional[str] = None
        arq_redis_url: Optional[str] = None
        queue_backend: str = "sync"
        require_redis_cache: bool = False
    
        # Configuration
        class Config:
            env_file = ".env"
            case_sensitive = False
            extra = "ignore"  # Ignore extra environment variables (Pydantic v2 compatibility)
    
        @property
        def redis_url_resolved(self) -> Optional[str]:
            """Resolve Redis URL with fallback logic"""
            return self.arq_redis_url or self.redis_url
    
    # ------------------------- Request Models (Pydantic) -------------------------
    class StandardErrorResponse(BaseModel):
        """Standardized error response format for consistent error handling"""
        error: str
        error_code: str
        error_details: Optional[Dict[str, Any]] = None
        retryable: bool = False
        user_action: Optional[str] = None
        timestamp: str = None
    
        def __init__(self, **data):
            if 'timestamp' not in data:
                # FIX #19: Use pendulum for consistent timezone handling
                import pendulum
                data['timestamp'] = pendulum.now().to_iso8601_string()
            super().__init__(**data)
    
    class FieldDetectionRequest(BaseModel):
        data: Dict[str, Any]
        filename: Optional[str] = None
        user_id: Optional[str] = None
        context: Optional[Dict[str, Any]] = None
    
    class PlatformDetectionRequest(BaseModel):
        payload: Optional[Dict[str, Any]] = None  # Structured data (columns, sample_data)
    class DocumentClassificationRequest(BaseModel):
        payload: Optional[Dict[str, Any]] = None
        filename: Optional[str] = None
        file_content: Optional[str] = None  # base64 or text content
        user_id: Optional[str] = None
        platform: Optional[str] = None
        document_type: Optional[str] = None  # New field added
        document_subtype: Optional[str] = None  # New field added
    
    # LIBRARY FIX: Pydantic models for metadata validation (replaces scattered JSON parsing)
    class UserConnectionMetadata(BaseModel):
        """Validated metadata for user_connections.metadata field"""
        last_history_id: Optional[str] = None  # Gmail incremental sync cursor
        last_synced_at: Optional[str] = None
        sync_errors: Optional[List[str]] = None
        error_count: int = 0
    
        class Config:
            extra = "allow"  # Allow additional fields for extensibility
    
    class SyncRunStats(BaseModel):
        """Validated stats for sync_runs.stats field"""
        records_fetched: int = 0
        actions_used: int = 0
        attachments_saved: int = 0
        queued_jobs: int = 0
        skipped: int = 0
    
        class Config:
            extra = "allow"
    
    # LIBRARY FIX: Connector-specific metadata and stats models for type-safe validation
    class ZohoMailMetadata(BaseModel):
        """Validated metadata for Zoho Mail user_connections.metadata field"""
        last_sync_token: Optional[str] = None  # Zoho incremental sync token
        last_synced_at: Optional[str] = None
        sync_errors: Optional[List[str]] = None
        error_count: int = 0
    
        class Config:
            extra = "allow"
    
    class XeroMetadata(BaseModel):
        """Validated metadata for Xero user_connections.metadata field"""
        last_sync_token: Optional[str] = None  # Xero incremental sync token
        last_synced_at: Optional[str] = None
        sync_errors: Optional[List[str]] = None
        error_count: int = 0
        tenant_id: Optional[str] = None  # Xero tenant ID for multi-tenant support
    
        class Config:
            extra = "allow"
    
    class StripeMetadata(BaseModel):
        """Validated metadata for Stripe user_connections.metadata field"""
        last_sync_token: Optional[str] = None  # Stripe pagination token
        last_synced_at: Optional[str] = None
        sync_errors: Optional[List[str]] = None
        error_count: int = 0
        account_id: Optional[str] = None  # Stripe account ID
    
        class Config:
            extra = "allow"
    
    class PayPalMetadata(BaseModel):
        """Validated metadata for PayPal user_connections.metadata field"""
        last_sync_token: Optional[str] = None  # PayPal pagination token
        last_synced_at: Optional[str] = None
        sync_errors: Optional[List[str]] = None
        error_count: int = 0
        merchant_id: Optional[str] = None  # PayPal merchant ID
    
        class Config:
            extra = "allow"
    
    class RazorpayMetadata(BaseModel):
        """Validated metadata for Razorpay user_connections.metadata field"""
        last_sync_token: Optional[str] = None  # Razorpay pagination token
        last_synced_at: Optional[str] = None
        sync_errors: Optional[List[str]] = None
        error_count: int = 0
        account_id: Optional[str] = None  # Razorpay account ID
    
        class Config:
            extra = "allow"
    
    class ConnectorSyncStats(BaseModel):
        """Extended stats for all connector syncs (Xero, Stripe, PayPal, Razorpay, etc.)"""
        records_fetched: int = 0
        actions_used: int = 0
        attachments_saved: int = 0
        queued_jobs: int = 0
        skipped: int = 0
        errors_encountered: int = 0
        processing_time_ms: Optional[int] = None
    
        class Config:
            extra = "allow"
    
    from supabase import create_client, Client
    import socket
    from urllib.parse import urlparse
    
    # Global lazy client instance (singleton)
    _supabase_client_instance: Optional[Client] = None
    _supabase_client_lock = threading.Lock()
    
    class LazySupabaseClient:
        """
        NUCLEAR FIX: Lazy proxy client that defers connection until first actual use.
        This prevents timeouts during initialization and allows app to start immediately.
        """
        def __init__(self, url: str, key: str):
            self.url = url
            self.key = key
            self._real_client = None
            self._connecting = False
            self._connect_lock = threading.Lock()
            self._connection_timeout = 5.0  # 5 second timeout
    
        def _ensure_connected(self):
            """Lazily connect on first actual API call with timeout"""
            if self._real_client is None and not self._connecting:
                with self._connect_lock:
                    if self._real_client is None:
                        try:
                            self._connecting = True
                            logger.info(f"\U0001f517 Lazy-connecting to Supabase on first use...")
    
                            # Connect in a thread with timeout to prevent hangs
                            client_holder = {'client': None, 'error': None}
    
                            def connect_thread():
                                try:
                                    client_holder['client'] = create_client(self.url, self.key)
                                except Exception as e:
                                    client_holder['error'] = e
    
                            thread = threading.Thread(target=connect_thread, daemon=True)
                            thread.start()
                            thread.join(timeout=self._connection_timeout)
    
                            if thread.is_alive():
                                logger.error(f"\u23f1\ufe0f Supabase connection timed out after {self._connection_timeout} seconds")
                                raise TimeoutError(f"Supabase connection timed out after {self._connection_timeout} seconds")
    
                            if client_holder['error']:
                                raise client_holder['error']
    
                            self._real_client = client_holder['client']
                            logger.info(f"\u2705 Lazy-connected to Supabase successfully")
                        except Exception as e:
                            logger.error(f"\u274c Failed to connect to Supabase on first use: {e}")
                            raise
                        finally:
                            self._connecting = False
    
        def __getattr__(self, name):
            """Proxy all attribute access to real client, connecting if needed"""
            self._ensure_connected()
            return getattr(self._real_client, name)
    
    
    def get_supabase_client(use_service_role: bool = True) -> Client:
        """
        Get a pooled Supabase client (singleton pattern with lazy connection).
    
        NUCLEAR FIX: Returns a lazy proxy that defers connection until first actual use.
        This prevents timeouts during initialization - the chat endpoint responds immediately.
    
        Returns:
            Lazy Supabase client instance (connects on first API call)
        """
        global _supabase_client_instance
    
        if _supabase_client_instance is None:
            with _supabase_client_lock:
                if _supabase_client_instance is None:
                    # ULTRA-FAST: Just read env vars directly, don't create connection pool
                    url = os.getenv('SUPABASE_URL')
                    key = (
                        os.getenv('SUPABASE_SERVICE_ROLE_KEY') or
                        os.getenv('SUPABASE_SERVICE_KEY')
                    )
    
                    if not url or not key:
                        logger.error("\u274c SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY not set")
                        raise ValueError("SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY (or SUPABASE_SERVICE_KEY) must be set")
    
                    # NUCLEAR FIX: Use lazy proxy instead of connecting immediately
                    _supabase_client_instance = LazySupabaseClient(url, key)
                    logger.info("\u2705 Created lazy Supabase client (will connect on first use)")
    
        return _supabase_client_instance
    
    # Supabase client is now inlined above - no external imports needed
    _supabase_import_errors = None
    from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST
    
    
    
    def get_queue_backend() -> str:
        """Return the queue backend mode: 'sync' or 'arq' (default)."""
        # Default to ARQ so background workers handle heavy processing.
        # Set QUEUE_BACKEND=sync in environments without an ARQ worker (e.g. local dev).
        from core_infrastructure.config_manager import get_queue_config
        return get_queue_config().backend.lower()
    
    # Global ARQ pool (singleton pattern for connection reuse)
    _arq_pool = None
    _arq_pool_lock = asyncio.Lock()
    
    async def get_arq_pool():
        """Get or create a singleton ARQ Redis pool using ARQ_REDIS_URL (or REDIS_URL)."""
        global _arq_pool
    
        # Fast path: pool already exists
        if _arq_pool is not None:
            return _arq_pool
    
        # Slow path: acquire lock and create pool
        async with _arq_pool_lock:
            # Double-check after acquiring lock (another thread might have created it)
            if _arq_pool is not None:
                return _arq_pool
    
            # Import inside function to avoid import overhead when not using ARQ
            from arq import create_pool
            from arq.connections import RedisSettings
            from core_infrastructure.config_manager import get_queue_config
            queue_cfg = get_queue_config()
            url = queue_cfg.redis_url
            if not url:
                raise RuntimeError("QUEUE_REDIS_URL not set for QUEUE_BACKEND=arq")
    
            _arq_pool = await create_pool(RedisSettings.from_dsn(url))
            logger.info(f"\u2705 ARQ connection pool created and cached for reuse")
            return _arq_pool
    
    # Using Groq/Llama exclusively for all AI operations
    # REPLACED: NangoClient \u2192 AirbytePythonClient (Airbyte handles OAuth and sync)
    from core_infrastructure.airbyte_client import AirbytePythonClient
    
    # Import critical fixes systems
    from core_infrastructure.transaction_manager import initialize_transaction_manager, get_transaction_manager
    from data_ingestion_normalization.streaming_processor import (
        initialize_streaming_processor,
        get_streaming_processor,
        StreamingConfig,
        StreamingFileProcessor,
    )
    from core_infrastructure.error_recovery_system import (
        initialize_error_recovery_system,
        get_error_recovery_system,
        ErrorContext,
        ErrorSeverity,
    )
    
    # CRITICAL FIX: Defer database_optimization_utils import to startup event
    # This import was blocking module load - moved to startup event where it's used
    from typing import TYPE_CHECKING
    if TYPE_CHECKING:
        from database_optimization_utils import OptimizedDatabaseQueries
    
    # Global optimized database client reference (set during startup)
    optimized_db: Optional["OptimizedDatabaseQueries"] = None
    
    # Global thread pool for CPU-bound operations (set during startup)
    _thread_pool: Optional[ThreadPoolExecutor] = None
    
    from core_infrastructure.centralized_cache import initialize_cache, get_cache, safe_get_cache
    
    # Backward compatibility alias
    safe_get_ai_cache = safe_get_cache
    
    import polars as pl
    
    # Import security system for input validation and protection
    from core_infrastructure.security_system import SecurityValidator, InputSanitizer, SecurityContext
    
    # REMOVED: Row hashing moved to duplicate detection service only
    # Backend no longer computes row hashes to avoid inconsistencies
    # Duplicate service handles all hashing via polars for consistency
    # from provenance_tracker import provenance_tracker, calculate_row_hash, create_lineage_path, append_lineage_step
    
    
    
    import structlog
    
    # Configure structlog for production-grade JSON logging
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer()
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        cache_logger_on_first_use=True,
    )
    
    # Use structlog as PRIMARY logger (not fallback)
    logger = structlog.get_logger(__name__)
    
    # Log any supabase_client import errors now that logger is initialized
    if _supabase_import_errors:
        logger.error(
            "supabase_client_import_failed",
            errors=_supabase_import_errors,
            hint="Supabase client is now inlined in fastapi_backend_v2.py - ensure SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY env vars are set"
        )
    elif get_supabase_client:
        logger.info("\u2705 supabase_client imported successfully")
    
    # Declare global variables
    security_validator = None
    structured_logger = logger  # Use structlog for all logging
    groq_client = None  # Global Groq client initialized in lifespan
    
    # ============================================================================
    # CRITICAL FIX: AidentMemoryManager LRU Cache (5-10x chat latency improvement)
    # ============================================================================
    # PROBLEM: Memory manager was instantiated on EVERY chat message (100-200ms latency)
    # SOLUTION: Cache memory managers by user_id with LRU eviction (maxsize=100 for 100 concurrent users)
    # IMPACT: 5-10x improvement in chat response times (100-200ms \u2192 10-20ms)
    
    from functools import lru_cache
    import os as _os
    
    @lru_cache(maxsize=100)
    def get_memory_manager(user_id: str) -> 'AidentMemoryManager':
        """
        Get or create cached memory manager for user.
    
        CRITICAL FIX: Eliminates 100-200ms initialization overhead per chat message.
        - First call: Creates new AidentMemoryManager (100-200ms)
        - Subsequent calls: Returns cached instance (< 1ms)
        - LRU eviction: Keeps 100 most recent users in memory
        - Per-user isolation: Each user gets their own isolated memory
    
        Args:
            user_id: Unique user identifier (cache key)
    
        Returns:
            Cached AidentMemoryManager instance for this user
        """
        try:
            from aident_cfo_brain.aident_memory_manager import AidentMemoryManager
        except ImportError:
            from aident_memory_manager import AidentMemoryManager
    
        redis_url = _os.getenv('ARQ_REDIS_URL') or _os.getenv('REDIS_URL')
        groq_key = _os.getenv('GROQ_API_KEY')
    
        memory_manager = AidentMemoryManager(
            user_id=user_id,
            redis_url=redis_url,
            groq_api_key=groq_key
        )
    
        logger.info(
            "memory_manager_cached",
            user_id=user_id,
            cache_info=get_memory_manager.cache_info()
        )
    
        return memory_manager
    
    # ----------------------------------------------------------------------------
    # Metrics (Prometheus) - Comprehensive business metrics
    # ----------------------------------------------------------------------------
    # Job/Task Metrics
    JOBS_ENQUEUED = Counter('jobs_enqueued_total', 'Jobs enqueued by provider and mode', ['provider', 'mode'])
    JOBS_PROCESSED = Counter('jobs_processed_total', 'Jobs processed by provider and status', ['provider', 'status'])
    ACTIVE_JOBS = Gauge('active_jobs_current', 'Number of currently active processing jobs')
    
    # Database Metrics
    DB_WRITES = Counter('db_writes_total', 'Database writes by table/op/status', ['table', 'op', 'status'])
    DB_WRITE_LATENCY = Histogram('db_write_latency_seconds', 'DB write latency seconds', ['table', 'op'])
    DB_READS = Counter('db_reads_total', 'Database reads by table/status', ['table', 'status'])
    
    # File Processing Metrics
    FILES_PROCESSED = Counter('files_processed_total', 'Total files processed by type and status', ['file_type', 'status'])
    PROCESSING_DURATION = Histogram('file_processing_duration_seconds', 'File processing duration in seconds', ['file_type'])
    SHEETS_PROCESSED = Counter('sheets_processed_total', 'Total sheets processed by status', ['status'])
    
    # OAuth Connector Metrics
    CONNECTOR_SYNCS = Counter('connector_syncs_total', 'Connector syncs by provider and status', ['provider', 'status'])
    CONNECTOR_SYNC_DURATION = Histogram('connector_sync_duration_seconds', 'Connector sync duration', ['provider'])
    CONNECTOR_ITEMS_FETCHED = Counter('connector_items_fetched_total', 'Items fetched by connector', ['provider'])
    
    # API Metrics
    API_REQUESTS = Counter('api_requests_total', 'API requests by endpoint and status', ['endpoint', 'status'])
    API_LATENCY = Histogram('api_latency_seconds', 'API request latency', ['endpoint'])
    
    # Normalization and entity pipeline metrics
    NORMALIZATION_EVENTS = Counter('normalization_events_total', 'Normalized events by provider', ['provider'])
    NORMALIZATION_DURATION = Histogram('normalization_duration_seconds', 'Normalization duration seconds', ['provider'])
    ENTITY_PIPELINE_RUNS = Counter('entity_pipeline_runs_total', 'Entity resolver runs by provider and status', ['provider', 'status'])
    
    # AI/ML Metrics
    AI_CLASSIFICATIONS = Counter('ai_classifications_total', 'AI classifications by model and status', ['model', 'status'])
    AI_CLASSIFICATION_DURATION = Histogram('ai_classification_duration_seconds', 'AI classification duration', ['model'])
    AI_CACHE_HITS = Counter('ai_cache_hits_total', 'AI cache hits vs misses', ['result'])
    
    # Error Metrics
    ERRORS_TOTAL = Counter('errors_total', 'Total errors by type and severity', ['error_type', 'severity'])
    RETRIES_TOTAL = Counter('retries_total', 'Total retries by operation', ['operation'])
    
    # ----------------------------------------------------------------------------
    # DB helper wrappers with metrics
    # ----------------------------------------------------------------------------
    # LIBRARY FIX #1: Use centralized sanitize_for_json from helpers.py
    # This function is now imported from core_infrastructure.utils.helpers
    # and handles all NaN/Inf/numpy scalar types without pandas dependency
    def _sanitize_for_json(obj):
        """Wrapper for centralized sanitize_for_json - delegates to helpers.py"""
        return sanitize_for_json(obj)
    
    def _db_insert(table: str, payload):
        t0 = time.time()
        try:
            # Sanitize payload to remove NaN/Inf values using centralized helper
            sanitized_payload = sanitize_for_json(payload)
            res = supabase.table(table).insert(sanitized_payload).execute()
            DB_WRITES.labels(table=table, op='insert', status='ok').inc()
            DB_WRITE_LATENCY.labels(table=table, op='insert').observe(max(0.0, time.time() - t0))
            return res
        except Exception as e:
            DB_WRITES.labels(table=table, op='insert', status='error').inc()
            DB_WRITE_LATENCY.labels(table=table, op='insert').observe(max(0.0, time.time() - t0))
            raise
    
    def _db_update(table: str, updates: dict, eq_col: str, eq_val):
        t0 = time.time()
        try:
            res = supabase.table(table).update(updates).eq(eq_col, eq_val).execute()
            DB_WRITES.labels(table=table, op='update', status='ok').inc()
            DB_WRITE_LATENCY.labels(table=table, op='update').observe(max(0.0, time.time() - t0))
            return res
        except Exception:
            DB_WRITES.labels(table=table, op='update', status='error').inc()
            DB_WRITE_LATENCY.labels(table=table, op='update').observe(max(0.0, time.time() - t0))
            raise
    
    # Import production duplicate detection service
    try:
        from duplicate_detection_fraud.production_duplicate_detection_service import (
            ProductionDuplicateDetectionService,
            FileMetadata,
            DuplicateType,
            DuplicateDetectionError
        )
        PRODUCTION_DUPLICATE_SERVICE_AVAILABLE = True
        logger.info("\u2705 Production duplicate detection service available")
    except ImportError as e:
        logger.warning(f"\u26a0\ufe0f Production duplicate detection service not available: {e}")
        PRODUCTION_DUPLICATE_SERVICE_AVAILABLE = False
    
    # Note: Legacy DuplicateDetectionService is defined below in this file
    
    # LIBRARY FIX #5: Removed duplicate OpenCV detection
    # OpenCV is now checked only in ADVANCED_FEATURES dict (lines ~1415-1420)
    # This eliminates redundant import and conflicting flags
    
    # FIX #18: Custom JSON encoder with orjson support for datetime objects
    import json
    class DateTimeEncoder(json.JSONEncoder):
        """
        Custom JSON encoder for handling datetime objects in API responses.
    
        Extends the standard JSONEncoder to properly serialize datetime objects
        to ISO format strings for API responses.
        """
        def default(self, obj):
            if isinstance(obj, datetime):
                return obj.isoformat()
            elif hasattr(obj, 'isoformat'):
                return obj.isoformat()
            return super().default(obj)
    
    def safe_json_dumps(obj, default=None):
        """
        orjson-based JSON serialization (3-5x faster than stdlib json)
        Hard dependency - no fallback to stdlib json.
    
        Benefits:
        - 3-5x faster serialization
        - Handles datetime objects via default parameter
        - Better performance for large objects
        - Consistent with safe_json_parse
        """
        try:
            serialized = serialize_datetime_objects(obj)
            return orjson.dumps(serialized).decode('utf-8')
        except TypeError as e:
            logger.error(f"orjson serialization failed - object not JSON serializable: {e}")
            raise ValueError(f"Object cannot be serialized to JSON: {e}") from e
        except Exception as e:
            logger.error(f"orjson serialization failed: {e}")
            raise
    
    # ============================================================================
    # HELPER FUNCTIONS - Consolidated in utils/helpers.py
    # ============================================================================
    # Moved to: core_infrastructure/utils/helpers.py
    # Imports below:
    from core_infrastructure.utils.helpers import (
        clean_jwt_token, safe_decode_base64, sanitize_for_json, get_groq_client,
        generate_friendly_status, send_websocket_progress,
        get_sync_cursor, save_sync_cursor, insert_external_item_with_error_handling
    )
    # NOTE: safe_openai_call removed - use instructor library for structured AI responses instead
    
    def safe_json_parse(json_str, fallback=None):
        """
        orjson-based JSON parsing (3-5x faster than standard json)
        Hard dependency - no fallback to stdlib json.
    
        Benefits:
        - 3-5x faster parsing
        - Better error messages
        - Handles Unicode correctly
        """
        if not json_str or not isinstance(json_str, str):
            return fallback
    
        try:
            cleaned = json_str.strip()
            if '```json' in cleaned:
                start = cleaned.find('```json') + 7
                end = cleaned.find('```', start)
                if end != -1:
                    cleaned = cleaned[start:end].strip()
            elif '```' in cleaned:
                start = cleaned.find('```') + 3
                end = cleaned.find('```', start)
                if end != -1:
                    cleaned = cleaned[start:end].strip()
    
            return orjson.loads(cleaned)
    
        except (orjson.JSONDecodeError, ValueError) as e:
            logger.error(f"JSON parsing failed: {e}")
            logger.error(f"Input string: {json_str[:200]}...")
            raise ValueError(f"Invalid JSON: {e}") from e
        except Exception as e:
            logger.error(f"Unexpected error in JSON parsing: {e}")
            raise
    
    # PHASE 3.1: pendulum for datetime (Better timezone handling)
    import pendulum
    
    def serialize_datetime_objects(obj):
        """
        PHASE 3.1: pendulum-based datetime serialization (Better timezone handling)
        Replaces 17 lines with pendulum's superior timezone support.
    
        Benefits:
        - Proper timezone handling (100+ formats)
        - Better parsing and formatting
        - Handles edge cases correctly
        """
        if isinstance(obj, datetime):
            # Convert to pendulum for proper timezone handling
            return pendulum.instance(obj).to_iso8601_string()
        elif hasattr(obj, 'isoformat'):
            return obj.isoformat()
        elif isinstance(obj, dict):
            return {key: serialize_datetime_objects(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [serialize_datetime_objects(item) for item in obj]
        elif isinstance(obj, tuple):
            return tuple(serialize_datetime_objects(item) for item in obj)
        else:
            return obj
    
    # Duplicate functions removed - using the first definitions above
    
    # ============================================================================
    # MESSAGE FORMATTING - "SENSE \u2192 UNDERSTAND \u2192 EXPLAIN \u2192 ACT" FRAMEWORK
    # ============================================================================
    
    class ProcessingStage:
        """Processing stages following cognitive flow: Sense \u2192 Understand \u2192 Explain \u2192 Act"""
        SENSE = "sense"          # Observing and reading data
        UNDERSTAND = "understand"  # Processing and analyzing
        EXPLAIN = "explain"      # Generating insights
        ACT = "act"             # Taking action and storing
    
    def format_progress_message(stage: str, action: str, details: str = None, count: int = None, total: int = None) -> str:
        """
        Format progress messages with emotional, personality-driven language.
        Finley is an AI employee - professional, helpful, and human-like.
    
        Args:
            stage: One of ProcessingStage values (sense, understand, explain, act)
            action: The specific action being performed
            details: Optional additional context
            count: Optional current count for progress tracking
            total: Optional total count for progress tracking
    
        Returns:
            Formatted message string with personality
    
        Examples:
            format_progress_message("sense", "Reading your file")
            -> "I'm reading your file now"
    
            format_progress_message("understand", "Matching vendor names", count=50, total=100)
            -> "I'm matching vendor names (50 of 100 done)"
    
            format_progress_message("explain", "Found patterns", details="3 duplicates detected")
            -> "I found patterns - 3 duplicates detected"
        """
        # Emotional, personality-driven prefixes
        # Finley speaks as "I" - a helpful AI employee
        stage_map = {
            ProcessingStage.SENSE: "I'm",        # "I'm reading..." (present continuous - active)
            ProcessingStage.UNDERSTAND: "I'm",   # "I'm analyzing..." (present continuous - thinking)
            ProcessingStage.EXPLAIN: "I",        # "I found..." (present simple - discovery)
            ProcessingStage.ACT: "I'm"          # "I'm saving..." (present continuous - action)
        }
    
        prefix = stage_map.get(stage, "I'm")
    
        # Make action lowercase for natural flow
        action_lower = action[0].lower() + action[1:] if action else action
        message = f"{prefix} {action_lower}"
    
        # Add count information with personality
        if count is not None and total is not None:
            message += f" ({count:,} of {total:,} done)"
        elif count is not None:
            message += f" ({count:,} completed)"
    
        # Add details if provided
        if details:
            message += f" - {details}"
    
        return message
    
    # CRITICAL: Declare global variables that will be initialized in startup event
    # These must be declared at module level before the startup event can modify them
    supabase = None
    optimized_db = None
    security_validator = None
    centralized_cache = None
    _supabase_loaded = False
    _supabase_lock = threading.Lock()
    
    def _ensure_supabase_loaded_sync():
        """
        Synchronous helper to lazy-load Supabase client on first use.
        This allows the application to start even if Supabase is temporarily unavailable,
        and initializes the connection only when actually needed.
    
        CRITICAL FIX: Uses inlined get_supabase_client() from this module (no external imports).
        """
        global supabase, _supabase_loaded
    
        if not _supabase_loaded:
            with _supabase_lock:
                if not _supabase_loaded:
                    try:
                        # Use the inlined get_supabase_client() function defined above (lines 374-404)
                        # This is now part of this module, no external imports needed
                        supabase = get_supabase_client()
                        _supabase_loaded = True
                        logger.info("\u2705 Supabase client lazy-loaded on first use")
                    except Exception as e:
                        logger.error(f"\u274c Failed to lazy-load Supabase client: {e.__class__.__name__}: {e}")
                        _supabase_loaded = True  # Mark as attempted to avoid repeated retries
                        supabase = None
    
        return supabase
    
    async def _ensure_supabase_loaded():
        """
        Async wrapper for lazy-loading Supabase client.
    
        NUCLEAR FIX: Returns immediately with lazy client that defers connection until first API call.
        No timeout needed since we're not actually connecting during this call.
        """
        try:
            # This returns immediately with a lazy proxy client
            return await asyncio.to_thread(_ensure_supabase_loaded_sync)
        except Exception as e:
            logger.error(f"\u274c Failed to create lazy Supabase client: {e.__class__.__name__}: {e}")
            return None
    
    # CRITICAL FIX: Define SocketIOWebSocketManager class before lifespan function
    # This was previously at line 11665 but needs to be here to avoid forward reference error
    class SocketIOWebSocketManager:
        """Socket.IO-based WebSocket manager - simplified with library handling"""
    
        def __init__(self):
            self.redis = None
            self.job_status: Dict[str, Dict[str, Any]] = {}  # In-memory cache
    
        def set_redis(self, redis_client):
            """Set Redis client for job state persistence"""
            self.redis = redis_client
            try:
                redis_manager = socketio.AsyncRedisManager(
                    f"redis://{redis_client.connection_pool.connection_kwargs.get('host', 'localhost')}:"
                    f"{redis_client.connection_pool.connection_kwargs.get('port', 6379)}"
                )
                sio.manager = redis_manager
                logger.info("\u2705 Socket.IO Redis adapter initialized")
            except Exception as e:
                logger.warning(f"Socket.IO Redis adapter failed: {e}")
    
        def _key(self, job_id: str) -> str:
            return f"finley:job:{job_id}"
    
        async def _get_state(self, job_id: str) -> Optional[Dict[str, Any]]:
            """FIX #14: Cache-only state retrieval with proper error handling"""
            try:
                cache = safe_get_cache()
                if cache is not None:
                    raw = await cache.get(self._key(job_id))
                    if raw:
                        # Cache already handles JSON serialization
                        state = raw if isinstance(raw, dict) else orjson.loads(raw)
                        return state
                # If cache unavailable or miss, return None (no fallback to memory)
                return None
            except Exception as e:
                logger.warning(f"Cache retrieval failed for job {job_id}: {e}")
                return None
    
        async def _save_state(self, job_id: str, state: Dict[str, Any]):
            """FIX #14: Cache-only state storage with explicit error handling"""
            cache = safe_get_cache()
            if cache is None:
                logger.error(f"\u274c CRITICAL: Cache unavailable - cannot persist job state {job_id}")
                raise RuntimeError(f"Cache service unavailable for job {job_id}")
    
            try:
                # Cache handles JSON serialization automatically, TTL in seconds
                await cache.set(self._key(job_id), state, ttl=21600)  # 6 hours
            except Exception as e:
                logger.error(f"\u274c CRITICAL: Failed to save job state {job_id}: {e}")
                raise RuntimeError(f"Failed to persist job state: {e}")
    
        async def merge_job_state(self, job_id: str, patch: Dict[str, Any]) -> Dict[str, Any]:
            base = await self._get_state(job_id) or {}
            base.update(patch)
            await self._save_state(job_id, base)
            return base
    
        async def send_update(self, job_id: str, data: Dict[str, Any]):
            """Send update via Socket.IO to job room"""
            try:
                await self.merge_job_state(job_id, data)
                payload = {**data, "job_id": job_id}
                await sio.emit('job_update', payload, room=job_id)
                return True
            except Exception as e:
                logger.error(f"Failed to send update for job {job_id}: {e}")
                return False
    
        async def send_error(self, job_id: str, error_message: str, component: str = None):
            """Send error via Socket.IO"""
            try:
                payload = {
                    "type": "error",
                    "job_id": job_id,
                    "error": error_message,
                    "component": component,
                    "timestamp": pendulum.now().to_iso8601_string()
                }
                await self.merge_job_state(job_id, {"status": "failed", "error": error_message})
                await sio.emit('job_error', payload, room=job_id)
                return True
            except Exception as e:
                logger.error(f"Failed to send error for job {job_id}: {e}")
                return False
    
        async def get_job_status(self, job_id: str) -> Optional[Dict[str, Any]]:
            """Get current job status"""
            return await self._get_state(job_id)
    
        async def send_overall_update(self, job_id: str, status: str, message: str, progress: Optional[int] = None, results: Optional[Dict[str, Any]] = None):
            """Send overall job update - wrapper for send_update with standardized payload"""
            payload = {
                "status": status,
                "message": message,
                "timestamp": pendulum.now().to_iso8601_string()
            }
            if progress is not None:
                payload["progress"] = progress
            if results is not None:
                payload["results"] = results
    
            await self.send_update(job_id, payload)
    
        async def send_component_update(self, job_id: str, component: str, status: str, message: str, progress: Optional[int] = None, data: Optional[Dict[str, Any]] = None):
            """Send component-specific update"""
            payload = {
                "component": component,
                "status": status,
                "message": message,
                "timestamp": pendulum.now().to_iso8601_string()
            }
            if progress is not None:
                payload["progress"] = progress
            if data is not None:
                payload["data"] = data
    
            await self.send_update(job_id, payload)
    
    # CRITICAL FIX: Initialize as None to prevent race condition during double import
    # Will be initialized in app_lifespan to avoid crash when Uvicorn + ARQ worker both import
    websocket_manager = None
    
    # CRITICAL FIX: Define lifespan before creating app so we can pass it to FastAPI constructor
    # This ensures proper startup/shutdown lifecycle management
    @asynccontextmanager
    async def app_lifespan(app: FastAPI):
        """Application lifespan context manager - handles startup and shutdown"""
        # Startup
        global supabase, optimized_db, security_validator, centralized_cache, websocket_manager, groq_client
    
        logger.info("="*80)
        logger.info("\U0001f680 STARTING SERVICE INITIALIZATION...")
        logger.info("="*80)
    
        # CRITICAL FIX: Initialize WebSocket manager here to avoid race condition
        # This prevents crash when both Uvicorn and ARQ worker import the module simultaneously
        try:
            websocket_manager = SocketIOWebSocketManager()
            logger.info("\u2705 WebSocket manager initialized")
        except Exception as ws_err:
            logger.error(f"\u274c Failed to initialize WebSocket manager: {ws_err}")
            websocket_manager = None
    
        try:
            # Try multiple possible environment variable names for Render compatibility
            supabase_url = (
                os.environ.get("SUPABASE_URL") or
                os.environ.get("SUPABASE_PROJECT_URL") or
                os.environ.get("DATABASE_URL")  # Sometimes Render uses this
            )
            supabase_key = (
                os.environ.get("SUPABASE_SERVICE_ROLE_KEY") or
                os.environ.get("SUPABASE_SERVICE_KEY") or  # This is what's in Render!
                os.environ.get("SUPABASE_KEY") or
                os.environ.get("SUPABASE_ANON_KEY")  # Fallback to anon key if service role not available
            )
    
            # Enhanced diagnostics for deployment debugging
            logger.info(f"\U0001f50d Environment diagnostics:")
            logger.info(f"   SUPABASE_URL present: {'\u2705' if supabase_url else '\u274c'}")
            logger.info(f"   SUPABASE_SERVICE_ROLE_KEY present: {'\u2705' if supabase_key else '\u274c'}")
            supabase_env_vars = sorted([k for k in os.environ.keys() if 'SUPABASE' in k.upper()])
            logger.info(f"   Available env vars: {supabase_env_vars}")
    
            if supabase_key:
                supabase_key = clean_jwt_token(supabase_key)
    
            if not supabase_url or not supabase_key:
                missing_vars = []
                if not supabase_url:
                    missing_vars.append("SUPABASE_URL")
                if not supabase_key:
                    missing_vars.append("SUPABASE_SERVICE_KEY (or SUPABASE_SERVICE_ROLE_KEY)")
                raise ValueError(f"Missing required environment variables: {', '.join(missing_vars)}. Please check your deployment configuration.")
    
            # CRITICAL FIX: Defer Supabase client initialization to first use
            # Don't block startup on database connection - it will be initialized on first API request
            # This prevents startup timeouts and allows graceful degradation if DB is temporarily unavailable
            supabase = get_supabase_client()
            logger.info("\u2705 Supabase client will be lazy-loaded on first use (non-blocking startup)")
    
            # Initialize critical systems (only if Supabase is available)
            if supabase:
                try:
                    initialize_transaction_manager(supabase)
                    initialize_streaming_processor(StreamingConfig(
                        chunk_size=1000,
                        memory_limit_mb=1600,  # Increased from 800MB to handle larger files safely
                        max_file_size_gb=10
                    ))
                    initialize_error_recovery_system(supabase)
                    logger.info("\u2705 Transaction, streaming, and error recovery systems initialized")
                except Exception as sys_err:
                    logger.warning(f"\u26a0\ufe0f Failed to initialize critical systems: {sys_err}")
            else:
                logger.warning("\u26a0\ufe0f Skipping critical system initialization - Supabase unavailable")
    
            # Initialize security system (observability removed - using structlog)
            try:
                security_validator = SecurityValidator()
                logger.info("\u2705 Security validator initialized")
            except Exception as sec_err:
                logger.warning(f"\u26a0\ufe0f Failed to initialize security validator: {sec_err}")
    
            # CRITICAL FIX: Initialize optimized database queries
            # Import here to avoid blocking module load
            if supabase:
                try:
                    from database_optimization_utils import OptimizedDatabaseQueries
                    optimized_db = OptimizedDatabaseQueries(supabase)
                    logger.info("\u2705 Optimized database queries initialized")
                except Exception as opt_err:
                    logger.warning(f"\u26a0\ufe0f Failed to initialize optimized database queries: {opt_err}")
                    optimized_db = None
            else:
                logger.warning("\u26a0\ufe0f Skipping optimized database initialization - Supabase unavailable")
                optimized_db = None
    
            logger.info("\u2705 Observability and security systems initialized")
    
            # REFACTORED: Initialize centralized Redis cache (replaces ai_cache_system.py)
            # This provides distributed caching across all workers and instances for true scalability
            redis_url = os.environ.get('ARQ_REDIS_URL') or os.environ.get('REDIS_URL')
            if redis_url:
                try:
                    centralized_cache = initialize_cache(
                        redis_url=redis_url,
                        default_ttl=7200  # 2 hours default TTL
                    )
                    logger.info("\u2705 Centralized Redis cache initialized - distributed caching across all workers!")
                except Exception as cache_err:
                    logger.warning(f"\u26a0\ufe0f Failed to initialize Redis cache: {cache_err} - Running without distributed cache")
                    centralized_cache = None
            else:
                logger.warning("\u26a0\ufe0f REDIS_URL not set - Running without distributed cache")
                centralized_cache = None
    
            # Initialize Groq client
            try:
                groq_api_key = os.environ.get('GROQ_API_KEY')
                if groq_api_key:
                    if Groq:
                        groq_client = Groq(api_key=groq_api_key)
                        logger.info("\u2705 Groq client initialized")
                    else:
                        logger.warning("\u26a0\ufe0f Groq library not available, skipping client initialization")
                else:
                    logger.warning("\u26a0\ufe0f GROQ_API_KEY not set, AI features will be disabled")
            except Exception as groq_err:
                logger.error(f"\u274c Failed to initialize Groq client: {groq_err}")
                groq_client = None
    
            # Initialize global thread pool for CPU-bound operations
            try:
                _thread_pool = ThreadPoolExecutor(max_workers=5)
                logger.info("\u2705 Global thread pool initialized for CPU-bound operations")
            except Exception as thread_err:
                logger.warning(f"\u26a0\ufe0f Failed to initialize global thread pool: {thread_err}")
                _thread_pool = None
    
            # PERMANENT FIX: Pre-load heavy ML models at startup (not during first chat request)
            # This prevents 1-2 minute delays on first message
            logger.info("\U0001f504 Pre-loading ML models for chat orchestrator...")
            try:
                # Import and initialize intent classifier + output guard (loads spacy + sentence-transformers)
                from aident_cfo_brain.intent_and_guard_engine import (
                    get_intent_classifier,
                    get_output_guard
                )
    
                # Pre-load spacy + sentence-transformers models in background thread
                # This prevents blocking the startup event
                def _preload_models():
                    try:
                        logger.info("   Loading intent classifier (spacy + sentence-transformers)...")
                        intent_classifier = get_intent_classifier()
                        logger.info("   \u2705 Intent classifier loaded")
    
                        logger.info("   Loading output guard (sentence-transformers)...")
                        output_guard = get_output_guard()
                        logger.info("   \u2705 Output guard loaded")
    
                        logger.info("\u2705 All ML models pre-loaded successfully - chat will be instant!")
                    except Exception as model_err:
                        logger.warning(f"\u26a0\ufe0f Failed to pre-load ML models: {model_err} - will load on first chat (may be slow)")
    
                # Run model loading in thread pool to avoid blocking startup
                if _thread_pool:
                    _thread_pool.submit(_preload_models)
                    logger.info("   Model pre-loading started in background...")
                else:
                    # Fallback: load synchronously if thread pool not available
                    _preload_models()
    
            except Exception as model_import_err:
                logger.warning(f"\u26a0\ufe0f Failed to import intent classifier: {model_import_err}")
    
            logger.info("\u2705 All critical systems and optimizations initialized successfully")
    
        except Exception as e:
            logger.error(f"\u274c Failed to initialize critical systems: {e}")
            supabase = None
            optimized_db = None
            # Log critical database failure for monitoring
            logger.critical(f"\U0001f6a8 DATABASE CONNECTION FAILED - System running in degraded mode: {e}")
            # Initialize minimal observability/logging to prevent NameError in endpoints
            try:
                # Fallback lightweight initialization (observability removed - using structlog)
                security_validator = SecurityValidator()
                logger.info("\u2705 Degraded mode security initialized (no database)")
            except Exception as init_err:
                logger.warning(f"\u26a0\ufe0f Failed to initialize degraded security systems: {init_err}")
    
        # Log final startup status
        logger.info("="*80)
        logger.info("\U0001f3af STARTUP COMPLETE - Service Status Summary:")
        logger.info(f"   Supabase: {'\u2705 Connected' if supabase else '\u274c Not initialized'}")
        logger.info(f"   Groq Client: {'\u2705 Ready' if groq_client else '\u274c Not initialized'}")
        logger.info(f"   Redis Cache: {'\u2705 Connected' if centralized_cache else '\u274c Not initialized'}")
        logger.info(f"   Optimized DB: {'\u2705 Ready' if optimized_db else '\u274c Not initialized'}")
        logger.info(f"   Security Validator: {'\u2705 Ready' if security_validator else '\u274c Not initialized'}")
        logger.info(f"   WebSocket Manager: {'\u2705 Ready' if websocket_manager else '\u274c Not initialized'}")
        logger.info("="*80)
    
        yield
        # Shutdown
        logger.info("\U0001f6d1 Application shutting down...")
        # Cleanup happens here if needed
    
    # Initialize FastAPI app with enhanced configuration and lifespan
    app = FastAPI(
        title="Finley AI Backend",
        version="1.0.0",
        description="Advanced financial data processing and AI-powered analysis platform",
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        lifespan=app_lifespan  # Use lifespan context manager for startup/shutdown
    )
    
    # ISSUE #10 FIX: Initialize slowapi rate limiter (Redis-backed, distributed)
    # Replaces custom rate limiting with battle-tested library
    try:
        from slowapi import Limiter, _rate_limit_exceeded_handler
        from slowapi.util import get_remote_address
        from slowapi.errors import RateLimitExceeded
    
        # Initialize limiter with Redis backend for distributed rate limiting
        limiter = Limiter(
            key_func=get_remote_address,
            storage_uri=os.getenv("REDIS_URL", "redis://localhost:6379"),
            default_limits=["100/minute"]  # Global default
        )
        app.state.limiter = limiter
        app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)
        logger.info("\u2705 slowapi rate limiter initialized with Redis backend")
    except ImportError:
        logger.warning("\u26a0\ufe0f slowapi not available, rate limiting disabled")
        limiter = None
    except Exception as e:
        logger.warning(f"\u26a0\ufe0f Failed to initialize slowapi: {e}, rate limiting disabled")
        limiter = None
    
    # IMPROVEMENT: Global exception handler for consistent error responses
    from fastapi.responses import JSONResponse
    from fastapi.exceptions import RequestValidationError
    
    @app.exception_handler(Exception)
    async def global_exception_handler(request: Request, exc: Exception):
        """
        Global exception handler to ensure all errors return StandardErrorResponse format
        This provides consistent error handling across the entire API
        """
        import traceback
    
        # Get full stack trace
        tb_str = ''.join(traceback.format_exception(type(exc), exc, exc.__traceback__))
    
        # Log with full details
        logger.error(f"\u274c UNHANDLED EXCEPTION on {request.method} {request.url.path}")
        logger.error(f"Exception type: {type(exc).__name__}")
        logger.error(f"Exception message: {str(exc)}")
        logger.error(f"Full traceback:\n{tb_str}")
    
        # Determine if error is retryable
        retryable = isinstance(exc, (TimeoutError, ConnectionError))
    
        return JSONResponse(
            status_code=500,
            content=StandardErrorResponse(
                error=f"{type(exc).__name__}: {str(exc)}",
                error_code="INTERNAL_ERROR",
                retryable=retryable,
                user_action="Please try again. If the problem persists, contact support."
            ).dict()
        )
    
    @app.exception_handler(RequestValidationError)
    async def validation_exception_handler(request: Request, exc: RequestValidationError):
        """Handle validation errors with StandardErrorResponse format"""
        return JSONResponse(
            status_code=422,
            content=StandardErrorResponse(
                error="Invalid request data",
                error_code="VALIDATION_ERROR",
                error_details={"errors": exc.errors()},
                retryable=False,
                user_action="Please check your request data and try again."
            ).dict()
        )
    
    @app.exception_handler(HTTPException)
    async def http_exception_handler(request: Request, exc: HTTPException):
        """Handle HTTP exceptions with StandardErrorResponse format"""
        return JSONResponse(
            status_code=exc.status_code,
            content=StandardErrorResponse(
                error=exc.detail,
                error_code=f"HTTP_{exc.status_code}",
                retryable=exc.status_code in [408, 429, 500, 502, 503, 504],
                user_action="Please try again later." if exc.status_code >= 500 else None
            ).dict()
        )
    
    # CORS middleware with environment-based configuration
    # Prevents CSRF attacks in production by restricting origins
    ALLOWED_ORIGINS = os.getenv('CORS_ALLOWED_ORIGINS', '*').split(',')
    if ALLOWED_ORIGINS == ['*']:
        logger.warning("\u26a0\ufe0f  CORS is configured with wildcard '*' - this should only be used in development!")
    else:
        logger.info(f"\u2705 CORS configured with specific origins: {ALLOWED_ORIGINS}")
    
    app.add_middleware(
        CORSMiddleware,
        allow_origins=ALLOWED_ORIGINS if ALLOWED_ORIGINS != ["*"] else ["*"],  # Configured via CORS_ALLOWED_ORIGINS env var
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
        expose_headers=["*"],  # Allow all response headers to be exposed
        max_age=3600,  # Cache preflight requests for 1 hour
    )
    
    # REMOVED: Old startup event - now using lifespan context manager in FastAPI constructor
    # The app_lifespan function handles all initialization
    
    # Initialize global config with pydantic-settings
    try:
>       app_config = AppConfig()

core_infrastructure\fastapi_backend_v2.py:1444: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

__pydantic_self__ = AppConfig(), _case_sensitive = None
_nested_model_default_partial_update = None, _env_prefix = None
_env_file = WindowsPath('.'), _env_file_encoding = None
_env_ignore_empty = None, _env_nested_delimiter = None
_env_nested_max_split = None, _env_parse_none_str = None
_env_parse_enums = None, _cli_prog_name = None, _cli_parse_args = None
_cli_settings_source = None, _cli_parse_none_str = None
_cli_hide_none_type = None, _cli_avoid_json = None, _cli_enforce_required = None
_cli_use_class_docs_for_groups = None, _cli_exit_on_error = None
_cli_prefix = None, _cli_flag_prefix_char = None, _cli_implicit_flags = None
_cli_ignore_unknown_args = None, _cli_kebab_case = None, _cli_shortcuts = None
_secrets_dir = None, values = {}

    def __init__(
        __pydantic_self__,
        _case_sensitive: bool | None = None,
        _nested_model_default_partial_update: bool | None = None,
        _env_prefix: str | None = None,
        _env_file: DotenvType | None = ENV_FILE_SENTINEL,
        _env_file_encoding: str | None = None,
        _env_ignore_empty: bool | None = None,
        _env_nested_delimiter: str | None = None,
        _env_nested_max_split: int | None = None,
        _env_parse_none_str: str | None = None,
        _env_parse_enums: bool | None = None,
        _cli_prog_name: str | None = None,
        _cli_parse_args: bool | list[str] | tuple[str, ...] | None = None,
        _cli_settings_source: CliSettingsSource[Any] | None = None,
        _cli_parse_none_str: str | None = None,
        _cli_hide_none_type: bool | None = None,
        _cli_avoid_json: bool | None = None,
        _cli_enforce_required: bool | None = None,
        _cli_use_class_docs_for_groups: bool | None = None,
        _cli_exit_on_error: bool | None = None,
        _cli_prefix: str | None = None,
        _cli_flag_prefix_char: str | None = None,
        _cli_implicit_flags: bool | None = None,
        _cli_ignore_unknown_args: bool | None = None,
        _cli_kebab_case: bool | Literal['all', 'no_enums'] | None = None,
        _cli_shortcuts: Mapping[str, str | list[str]] | None = None,
        _secrets_dir: PathType | None = None,
        **values: Any,
    ) -> None:
>       super().__init__(
            **__pydantic_self__._settings_build_values(
                values,
                _case_sensitive=_case_sensitive,
                _nested_model_default_partial_update=_nested_model_default_partial_update,
                _env_prefix=_env_prefix,
                _env_file=_env_file,
                _env_file_encoding=_env_file_encoding,
                _env_ignore_empty=_env_ignore_empty,
                _env_nested_delimiter=_env_nested_delimiter,
                _env_nested_max_split=_env_nested_max_split,
                _env_parse_none_str=_env_parse_none_str,
                _env_parse_enums=_env_parse_enums,
                _cli_prog_name=_cli_prog_name,
                _cli_parse_args=_cli_parse_args,
                _cli_settings_source=_cli_settings_source,
                _cli_parse_none_str=_cli_parse_none_str,
                _cli_hide_none_type=_cli_hide_none_type,
                _cli_avoid_json=_cli_avoid_json,
                _cli_enforce_required=_cli_enforce_required,
                _cli_use_class_docs_for_groups=_cli_use_class_docs_for_groups,
                _cli_exit_on_error=_cli_exit_on_error,
                _cli_prefix=_cli_prefix,
                _cli_flag_prefix_char=_cli_flag_prefix_char,
                _cli_implicit_flags=_cli_implicit_flags,
                _cli_ignore_unknown_args=_cli_ignore_unknown_args,
                _cli_kebab_case=_cli_kebab_case,
                _cli_shortcuts=_cli_shortcuts,
                _secrets_dir=_secrets_dir,
            )
        )
E       pydantic_core._pydantic_core.ValidationError: 2 validation errors for AppConfig
E       supabase_url
E         Field required [type=missing, input_value={'openai_api_key': 'sk-pr...MuuzdHlX9C1vJ0hfYroRIc'}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.12/v/missing
E       SUPABASE_KEY
E         Field required [type=missing, input_value={'openai_api_key': 'sk-pr...MuuzdHlX9C1vJ0hfYroRIc'}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.12/v/missing

..\..\..\AppData\Local\Programs\Python\Python310\lib\site-packages\pydantic_settings\main.py:194: ValidationError
------------------------------- Captured stdout -------------------------------
[WARNING] glom not installed - nested data extraction will use fallback
[DEBUG] Importing UniversalFieldDetector...
[DEBUG] Importing UniversalPlatformDetector...
[DEBUG] Importing UniversalDocumentClassifier...
[DEBUG] Importing UniversalExtractors...
[DEBUG] Importing EntityResolver...
[DEBUG] Importing StreamedFile...
[DEBUG] Importing EnhancedRelationshipDetector...
[DEBUG] EnhancedRelationshipDetector imported successfully
[DEBUG] Importing ProvenanceTracker...
[DEBUG] ProvenanceTracker imported successfully
[DEBUG] Importing FieldMappingLearner...
[DEBUG] FieldMappingLearner imported successfully (nested)
============================== warnings summary ===============================
..\..\..\AppData\Local\Programs\Python\Python310\lib\site-packages\gotrue\types.py:679: 19 warnings
  C:\Users\91937\AppData\Local\Programs\Python\Python310\lib\site-packages\gotrue\types.py:679: PydanticDeprecatedSince20: The `update_forward_refs` method is deprecated; use `model_rebuild` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    model.update_forward_refs()

core_infrastructure\fastapi_backend_v2.py:10249
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\core_infrastructure\fastapi_backend_v2.py:10249: DeprecationWarning: invalid escape sequence '\,'
    """

data_ingestion_normalization\universal_field_detector.py:50
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\data_ingestion_normalization\universal_field_detector.py:50: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator('patterns')

data_ingestion_normalization\universal_field_detector.py:65
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\data_ingestion_normalization\universal_field_detector.py:65: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class FieldDetectorConfig(BaseSettings):

data_ingestion_normalization\universal_platform_detector_optimized.py:35
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\data_ingestion_normalization\universal_platform_detector_optimized.py:35: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class PlatformDetectorConfig(BaseSettings):

data_ingestion_normalization\universal_platform_detector_optimized.py:59
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\data_ingestion_normalization\universal_platform_detector_optimized.py:59: PydanticDeprecatedSince20: `min_items` is deprecated and will be removed, use `min_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    indicators: List[str] = Field(min_items=1, max_items=50)

data_ingestion_normalization\universal_platform_detector_optimized.py:59
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\data_ingestion_normalization\universal_platform_detector_optimized.py:59: PydanticDeprecatedSince20: `max_items` is deprecated and will be removed, use `max_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    indicators: List[str] = Field(min_items=1, max_items=50)

data_ingestion_normalization\universal_platform_detector_optimized.py:63
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\data_ingestion_normalization\universal_platform_detector_optimized.py:63: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator('indicators')

data_ingestion_normalization\universal_platform_detector_optimized.py:55
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\data_ingestion_normalization\universal_platform_detector_optimized.py:55: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class PlatformDefinition(BaseModel):

data_ingestion_normalization\universal_document_classifier_optimized.py:52
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\data_ingestion_normalization\universal_document_classifier_optimized.py:52: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class DocumentClassifierConfig(BaseSettings):

data_ingestion_normalization\universal_document_classifier_optimized.py:78
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\data_ingestion_normalization\universal_document_classifier_optimized.py:78: PydanticDeprecatedSince20: `min_items` is deprecated and will be removed, use `min_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    indicators: List[str] = Field(min_items=1, max_items=100)

data_ingestion_normalization\universal_document_classifier_optimized.py:78
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\data_ingestion_normalization\universal_document_classifier_optimized.py:78: PydanticDeprecatedSince20: `max_items` is deprecated and will be removed, use `max_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    indicators: List[str] = Field(min_items=1, max_items=100)

data_ingestion_normalization\universal_document_classifier_optimized.py:80
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\data_ingestion_normalization\universal_document_classifier_optimized.py:80: PydanticDeprecatedSince20: `min_items` is deprecated and will be removed, use `min_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    keywords: List[str] = Field(min_items=1, max_items=20)

data_ingestion_normalization\universal_document_classifier_optimized.py:80
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\data_ingestion_normalization\universal_document_classifier_optimized.py:80: PydanticDeprecatedSince20: `max_items` is deprecated and will be removed, use `max_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    keywords: List[str] = Field(min_items=1, max_items=20)

data_ingestion_normalization\universal_document_classifier_optimized.py:83
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\data_ingestion_normalization\universal_document_classifier_optimized.py:83: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator('indicators', 'keywords')

data_ingestion_normalization\universal_document_classifier_optimized.py:74
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\data_ingestion_normalization\universal_document_classifier_optimized.py:74: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class DocumentTypeDefinition(BaseModel):

data_ingestion_normalization\universal_extractors_optimized.py:51
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\data_ingestion_normalization\universal_extractors_optimized.py:51: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class ExtractorConfig(BaseSettings):

data_ingestion_normalization\entity_resolver_optimized.py:56
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\data_ingestion_normalization\entity_resolver_optimized.py:56: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class ScoredLabel(BaseModel):

data_ingestion_normalization\entity_resolver_optimized.py:65
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\data_ingestion_normalization\entity_resolver_optimized.py:65: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class ResolutionConfig(BaseSettings):

data_ingestion_normalization\entity_resolver_optimized.py:84
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\data_ingestion_normalization\entity_resolver_optimized.py:84: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class AIEntityDecision(BaseModel):

duplicate_detection_fraud\production_duplicate_detection_service.py:147
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\duplicate_detection_fraud\production_duplicate_detection_service.py:147: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class DuplicateServiceConfig(BaseSettings):

aident_cfo_brain\enhanced_relationship_detector.py:128
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\aident_cfo_brain\enhanced_relationship_detector.py:128: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator('semantic_description')

aident_cfo_brain\enhanced_relationship_detector.py:134
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\aident_cfo_brain\enhanced_relationship_detector.py:134: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator('reasoning')

aident_cfo_brain\enhanced_relationship_detector.py:150
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\aident_cfo_brain\enhanced_relationship_detector.py:150: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator('source_event_id', 'target_event_id')

aident_cfo_brain\enhanced_relationship_detector.py:156
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\aident_cfo_brain\enhanced_relationship_detector.py:156: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator('confidence_score')

core_infrastructure\fastapi_backend_v2.py:235
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\core_infrastructure\fastapi_backend_v2.py:235: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class AppConfig(BaseSettings):

core_infrastructure\fastapi_backend_v2.py:306
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\core_infrastructure\fastapi_backend_v2.py:306: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class UserConnectionMetadata(BaseModel):

core_infrastructure\fastapi_backend_v2.py:316
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\core_infrastructure\fastapi_backend_v2.py:316: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class SyncRunStats(BaseModel):

core_infrastructure\fastapi_backend_v2.py:328
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\core_infrastructure\fastapi_backend_v2.py:328: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class ZohoMailMetadata(BaseModel):

core_infrastructure\fastapi_backend_v2.py:338
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\core_infrastructure\fastapi_backend_v2.py:338: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class XeroMetadata(BaseModel):

core_infrastructure\fastapi_backend_v2.py:349
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\core_infrastructure\fastapi_backend_v2.py:349: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class StripeMetadata(BaseModel):

core_infrastructure\fastapi_backend_v2.py:360
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\core_infrastructure\fastapi_backend_v2.py:360: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class PayPalMetadata(BaseModel):

core_infrastructure\fastapi_backend_v2.py:371
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\core_infrastructure\fastapi_backend_v2.py:371: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class RazorpayMetadata(BaseModel):

core_infrastructure\fastapi_backend_v2.py:382
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\core_infrastructure\fastapi_backend_v2.py:382: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class ConnectorSyncStats(BaseModel):

core_infrastructure\config_manager.py:16
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\core_infrastructure\config_manager.py:16: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class ConnectorConfig(BaseSettings):

core_infrastructure\config_manager.py:65
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\core_infrastructure\config_manager.py:65: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class QueueConfig(BaseSettings):

core_infrastructure\config_manager.py:80
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\core_infrastructure\config_manager.py:80: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class AirbytePythonConfig(BaseSettings):

core_infrastructure\config_manager.py:98
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\core_infrastructure\config_manager.py:98: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class AppConfig(BaseSettings):

core_infrastructure\security_system.py:60
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\core_infrastructure\security_system.py:60: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator('timestamp', pre=True, always=True)

core_infrastructure\security_system.py:46
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\core_infrastructure\security_system.py:46: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class SecurityViolation(BaseModel):  # v4.0: pydantic for validation

core_infrastructure\security_system.py:65
  C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\core_infrastructure\security_system.py:65: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class SecurityContext(BaseModel):  # v4.0: pydantic for validation

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
ERROR tests/test_ingestion_phases_1_to_5.py - pydantic_core._pydantic_core.Va...
================== 59 warnings, 1 error in 175.09s (0:02:55) ==================
pytest : ERROR: not found: C:\Users\91937\Documents\GitHub\friendly-greetings-launchpad\tests\test_ingestion_phases_1_t
o_5.py::TestPhase1Controller::test_process_excel_endpoint_reachable
At line:1 char:1
+ pytest tests/test_ingestion_phases_1_to_5.py::TestPhase1Controller::t ...
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : NotSpecified: (ERROR: not foun...point_reachable:String) [], RemoteException
    + FullyQualifiedErrorId : NativeCommandError
 
(no name 'C:\\Users\\91937\\Documents\\GitHub\\friendly-greetings-launchpad\\tests\\test_ingestion_phases_1_to_5.py::Te
stPhase1Controller::test_process_excel_endpoint_reachable' in any of [<Module tests/test_ingestion_phases_1_to_5.py>])

